{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Classification by regional modeling</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification by regional modeling consists in a five-step approach:\n",
    "1. Setting the hyper-parameters. In this step, we specify the number of SOM prototypes $C$. It must be also defined as\n",
    "the maximum number of regions $K_{max}$. Without any prior knowledge, we will set in this example $K_{max} = \\sqrt{C}$.\n",
    "\n",
    "\n",
    "2. SOM training. In order to build regional models, follow the procedure introduced by Vesanto and Alhoniemi [1].\n",
    "Thus, the very first step requires training the SOM as usual, with $C$ prototypes.\n",
    "\n",
    "\n",
    "3. Clustering of the SOM. The step consists in performing clustering over the $C$ SOM prototypes. Although one may\n",
    "use any clustering algorithm for this step, for the sake of simplicity, we use the standard K-means algorithm in\n",
    "combination with the Davies–Bouldin (DB) index. The DB index is a clustering validity measure commonly used for\n",
    "finding the optimal number of clusters, but any suitable measure can be equally used (see [2]). Thus, we compute\n",
    "$K = 1, 2, ... K_{max}$ partitioning of the SOM prototypes and the corresponding DB index value as well.\n",
    "The optimal partitioning, represented by $K_{opt}$ partitions, is then the value of $K$ wich minimizes the DB index.\n",
    "\n",
    "\n",
    "4. Partitioning SOM prototypes into regions. Once $K_{opt}$ is selected, the $r$-th cluster of SOM prototypes,\n",
    "$r = 1...K_{opt}$, is composed of all weight vectors $w_i$ that are mapped onto the prototype $p_r$ of the K-means\n",
    "algorithm. More formally, the set of SOM prototypes associated with the r-th prototype of the K-means algorithm\n",
    "is defined as: $$W_r = \\{w_i \\in R^{p+q} | \\|w_i-p_r\\| < \\|w_i-p_j\\|, \\forall j =1,...,K_{opt}, j\\neq r \\}$$\n",
    "\n",
    "\n",
    "5. Mapping data points to regions. The fourth step consists in finding $K_{opt}$ data partitions, denoted by\n",
    "$\\{X_1\\}$, $\\{X_2\\}$, ... , $\\{X_{K_{opt}}\\}$ of the training dataset by mapping each datapoint to a region\n",
    "$r \\in \\{1, ... , K_{opt}\\}$. In other words, let us denote $N_r$ as the number of data vectors in $\\{X_r\\}$.\n",
    "Then, the partition $\\{X_r\\}$ is composed of those input vectors $x_{rμ}$, $μ = 1, ... , N_r$ , whose closest SOM\n",
    "prototype belongs to $W_r$.\n",
    "\n",
    "\n",
    "6. Building classification models over the regions. Finally, once the original dataset has been divided into $K_{opt}$\n",
    "subsets (one per region), the last step consists in building $K_{opt}$ regional classification models using\n",
    "$X_r$, $r = 1, ... , K_{opt}$.\n",
    "\n",
    "* Vertebral Column\n",
    "* Wall-Following\n",
    "* Alzheimer\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "[1] J. Vesanto, E. Alhoniemi, Clustering of the self-organizing map, IEEE Trans.\n",
    "Neural Netw. 11 (2000) 586–600.\n",
    "\n",
    "[2] M. Halkidi, Y. Batistakis, M. Vazirgiannis, On clustering validation techniques, J. Intell. Inf. Syst. 17 (2001) 107–145."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  Features.shape:   # of classes:\n",
      "vc2c      (310, 6)          2\n",
      "vc3c      (310, 6)          3\n",
      "wf24f     (5456, 24)        4\n",
      "wf4f      (5456, 4)         4\n",
      "wf2f      (5456, 2)         4\n",
      "pk        (195, 22)         2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.9.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.56 s\n",
      "Wall time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from math import ceil\n",
    "from load_dataset import datasets\n",
    "\n",
    "from devcode.utils import dummie2multilabel, scale_feat\n",
    "from devcode.models.som import SOM\n",
    "from devcode.models.regional_learning import RegionalModel\n",
    "from devcode import run_round\n",
    "\n",
    "from devcode.utils.metrics import DB\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "dataset_name='pk'\n",
    "\n",
    "X = datasets[dataset_name]['features'].values\n",
    "Y = datasets[dataset_name]['labels'].values\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "# Train/Test split = 80%/20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=test_size)\n",
    "\n",
    "# scaling features\n",
    "X_tr_norm, X_ts_norm = scale_feat(X_train, X_test, scaleType='min-max')\n",
    "\n",
    "#N = len(dataset['features'].index) # number of datapoints\n",
    "N = len(X_tr_norm) # number of datapoints in the train split\n",
    "l = ceil((5*N**.5)**.5) # side length of square grid of neurons\n",
    "\n",
    "som = SOM(l,l)\n",
    "som_params={\n",
    "    'alpha0':    0.01,\n",
    "    'sigma0':    1,\n",
    "    'nEpochs':   1,\n",
    "    'verboses':  0            \n",
    "}\n",
    "\n",
    "C = l**2 # number of SOM neurons in the 2D grid\n",
    "k_values = [i for i in range(2, ceil(np.sqrt(C)))] # 2 to sqrt(C)\n",
    "cluster_params={\n",
    "    'n_clusters': {'metric':   DB,        # when a dictionary is pass a search begins\n",
    "                   'criteria': np.argmin, # search for smallest DB score \n",
    "                   'k_values': k_values}, # around the values provided in 'k_values'\n",
    "    'n_init':     10, # number of initializations\n",
    "    'init':       'random', \n",
    "    #'n_jobs':     -1\n",
    "}\n",
    "\n",
    "linearModel = linear_model.LinearRegression(n_jobs=-1)\n",
    "\n",
    "rm = RegionalModel(som, linearModel)\n",
    "rm.fit(X=X_tr_norm, Y=y_train, verboses=0,\n",
    "        SOM_params     = som_params,\n",
    "        Cluster_params = cluster_params)\n",
    "\n",
    "# Evaluating in the test dataset\n",
    "y_pred = rm.predict(X_ts_norm)\n",
    "y_pred = np.round(np.clip(y_pred, 0, 1)) # rounding prediction numbers\n",
    "\n",
    "cm = confusion_matrix(dummie2multilabel(y_test),\n",
    "                      dummie2multilabel(y_pred))\n",
    "#cm = np.asarray(cm).reshape(-1) # matrix => array\n",
    "acc=0\n",
    "total=sum(sum(cm))\n",
    "for j in range(len(cm)):\n",
    "    acc += cm[j,j] # summing the diagonal\n",
    "acc/=total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8461538461538461"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'labels_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_round\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRegionalModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSOM_class\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModel_class\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinearModel\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_rounded\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Portfolio\\Research\\regional-classifiers\\devcode\\__init__.py:45\u001b[0m, in \u001b[0;36mrun_round\u001b[1;34m(X, y, test_size, method, tr_params, is_rounded)\u001b[0m\n\u001b[0;32m     42\u001b[0m X_tr_norm, X_ts_norm \u001b[38;5;241m=\u001b[39m scale_feat(X_train, X_test, scaleType\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin-max\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtr_params)\n\u001b[1;32m---> 45\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m y_pred_tr \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_tr_norm)\n\u001b[0;32m     48\u001b[0m y_pred_ts \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_ts_norm)\n",
      "File \u001b[1;32mD:\\Portfolio\\Research\\regional-classifiers\\devcode\\models\\regional_learning.py:67\u001b[0m, in \u001b[0;36mRegionalModel.fit\u001b[1;34m(self, X, Y, verboses, SOM_params, Cluster_params, Model_params)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCluster \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mk_opt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSOM\u001b[38;5;241m.\u001b[39mneurons)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Model training\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregion_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregionalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# finding labels of datapoints\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verboses \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart of Model training at \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()))\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregional_models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m k_opt\n",
      "File \u001b[1;32mD:\\Portfolio\\Research\\regional-classifiers\\devcode\\models\\regional_learning.py:93\u001b[0m, in \u001b[0;36mRegionalModel.regionalize\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)):  \u001b[38;5;66;03m# for each datapoint\u001b[39;00m\n\u001b[0;32m     91\u001b[0m     winner_som_idx, dist_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSOM\u001b[38;5;241m.\u001b[39mget_winner(\n\u001b[0;32m     92\u001b[0m         X[i], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dist_matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# find closest neuron\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     region \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCluster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels_\u001b[49m[winner_som_idx]  \u001b[38;5;66;03m# find neuron label index in kmeans\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# if the region don't have a model is because it didn't have datapoints in the train set\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m region \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mempty_regions:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'labels_'"
     ]
    }
   ],
   "source": [
    "run_round(X, Y, test_size, RegionalModel, {\"SOM_class\": som, \"Model_class\": linearModel}, is_rounded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of regional OLS in the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant hyperparameters:\n",
    "test_size = 0.2\n",
    "scaleType = 'min-max'\n",
    "n_resamplings = 100\n",
    "\n",
    "# hyperparameters grid search:\n",
    "num = 3\n",
    "alphas = np.linspace(0.1, 0.5,  num=num).tolist()\n",
    "sigmas = np.linspace(3,    10,   num=num).tolist()\n",
    "epochs = np.linspace(100,  500, num=num, dtype='int').tolist()\n",
    "\n",
    "# vector of random states for train/test split\n",
    "random_states = np.random.randint(np.iinfo(np.int32).max, size=n_resamplings).tolist()\n",
    "cases = [\n",
    "    {\n",
    "         \"dataset_name\" : dataset_name\n",
    "        ,\"random_state\":  random_state\n",
    "        ,\"som_params\":    { \"alpha0\"  : alpha0\n",
    "                           ,\"sigma0\"  : sigma0\n",
    "                           ,\"nEpochs\" : nEpochs\n",
    "                          }\n",
    "    }\n",
    "    # hyperparameters possible values\n",
    "    for dataset_name in datasets.keys()\n",
    "    for random_state in random_states\n",
    "    for alpha0       in alphas\n",
    "    for sigma0       in sigmas\n",
    "    for nEpochs      in epochs\n",
    "]\n",
    "\n",
    "print(\"alphas: {}\\nsigmas: {}\\nepochs: {}\\n\".format(alphas,sigmas,epochs))\n",
    "\n",
    "print(\"# of alphas: {}\\n# of sigmas: {}\\n# of epochs: {}\\n# of random_states: {}\\n# of datasets: {}\\n\".format(\n",
    "    len(alphas), len(sigmas), len(epochs), len(random_states), len(list(datasets.keys()))))\n",
    "\n",
    "print(\"# of cases: {}\".format(len(cases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def evalRLM(case):\n",
    "    dataset_name = case['dataset_name']\n",
    "    random_state = case['random_state']\n",
    "    som_params   = case['som_params']\n",
    "    \n",
    "    X = datasets[dataset_name]['features'].values\n",
    "    Y = datasets[dataset_name]['labels'].values\n",
    "    \n",
    "    # Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=test_size, random_state=random_state)\n",
    "    # scaling features\n",
    "    X_tr_norm, X_ts_norm = scale_feat(X_train, X_test, scaleType=scaleType)\n",
    "\n",
    "    N = len(X_tr_norm) # number of datapoints in the train split\n",
    "    l = ceil((5*N**.5)**.5) # side length of square grid of neurons\n",
    "\n",
    "    som = SOM(l,l)\n",
    "\n",
    "    C = l**2 # number of SOM neurons in the 2D grid\n",
    "    k_values = [i for i in range(2, ceil(np.sqrt(C)))] # 2 to sqrt(C)\n",
    "    cluster_params={\n",
    "        'n_clusters': {'metric':   DB,        # when a dictionary is pass a search begins\n",
    "                       'criteria': np.argmin, # search for smallest DB score \n",
    "                       'k_values': k_values}, # around the values provided in 'k_values'\n",
    "        'n_init':     10, # number of initializations\n",
    "        'init':       'random'\n",
    "        #'n_jobs':     0\n",
    "    }\n",
    "\n",
    "    linearModel = linear_model.LinearRegression()\n",
    "\n",
    "    rlm = RegionalModel(som, linearModel)\n",
    "    rlm.fit(X=X_tr_norm, Y=y_train,\n",
    "            SOM_params     = som_params,\n",
    "            Cluster_params = cluster_params)\n",
    "\n",
    "    # Evaluating in the train set\n",
    "    y_tr_pred = rlm.predict(X_tr_norm)\n",
    "    y_tr_pred = np.round(np.clip(y_tr_pred, 0, 1)) # rounding prediction numbers\n",
    "\n",
    "    cm_tr = confusion_matrix(dummie2multilabel(y_train),\n",
    "                             dummie2multilabel(y_tr_pred)\n",
    "                            ).reshape(-1) # matrix => array\n",
    "\n",
    "    # Evaluating in the test set\n",
    "    y_ts_pred = rlm.predict(X_ts_norm)\n",
    "    y_ts_pred = np.round(np.clip(y_ts_pred, 0, 1)) # rounding prediction numbers\n",
    "\n",
    "    cm_ts = confusion_matrix(dummie2multilabel(y_test),\n",
    "                             dummie2multilabel(y_ts_pred)\n",
    "                            ).reshape(-1) # matrix => array\n",
    "\n",
    "    data = [dataset_name, random_state]+list(som_params.values())+[cm_tr]+[cm_ts]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "#cm_size = (Y.shape[1]+1) # number of rows/columns in the confusion matrix\n",
    "#header = ['set']+list(cases[0].keys())+['C({},{})'.format(i,j) for i in range(cm_size) \n",
    "#                                                               for j in range(cm_size)]\n",
    "\n",
    "pool = Pool() # Create a multiprocessing Pool\n",
    "data = pool.map(evalRLM, cases[0:2]) # process data_inputs iterable with pool\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "results = np.vstack(data)\n",
    "header  = [\"dataset_name\", \"random_state\", \"alpha0\", \"sigma0\", \"nEpochs\", \"cm_tr\", \"cm_ts\"]\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=header)\n",
    "\n",
    "filename = \"ROLS - all - n_res={n_resamplings} - {datetime}.csv\".format(\n",
    "    n_resamplings=n_resamplings,\n",
    "    datetime=datetime.datetime.now()\n",
    ")\n",
    "results_df.to_csv(filename,index=False) # saving results in csv file\n",
    "\n",
    "\n",
    "# 2019-03-13 19:22:45.669104\n",
    "# 2019-03-18 01:43:55.780175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browser notification when cell finishs with '%%notify'\n",
    "# import jupyternotify\n",
    "# ip = get_ipython()\n",
    "# ip.register_magics(jupyternotify.JupyterNotifyMagics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%notify` not found.\n"
     ]
    }
   ],
   "source": [
    "%%notify\n",
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "\n",
    "data = [None]*len(cases)\n",
    "count=0\n",
    "pool = Pool()\n",
    "for i in tqdm.tqdm(pool.imap_unordered(evalRLM, cases), total=len(cases)):\n",
    "    data[count] = i\n",
    "    count+=1\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "\n",
    "results = np.vstack(data)\n",
    "header  = [\"dataset_name\", \"random_state\", \"alpha0\", \"sigma0\", \"nEpochs\", \"cm_tr\", \"cm_ts\"]\n",
    "results_df = pd.DataFrame(results, columns=header)\n",
    "\n",
    "filename = \"ROLS - all - n_res={n_resamplings} - {datetime}.csv\".format(\n",
    "    n_resamplings=n_resamplings,\n",
    "    datetime=datetime.datetime.now()\n",
    ")\n",
    "results_df.to_csv(filename,index=False) # saving results in csv file\n",
    "\n",
    "# [{elapsed}<{remaining}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Processing results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'ROLS - all - n_res=100 - 2019-07-10 04:50:57.404253.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# loading simulation results\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_results \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mROLS - all - n_res=100 - 2019-07-10 04:50:57.404253.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df_results\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'ROLS - all - n_res=100 - 2019-07-10 04:50:57.404253.csv'"
     ]
    }
   ],
   "source": [
    "# loading simulation results\n",
    "df_results = pd.read_csv('ROLS - all - n_res=100 - 2019-07-10 04:50:57.404253.csv')\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for each data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alphas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m som_params \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     {\n\u001b[0;32m      3\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha0\u001b[39m\u001b[38;5;124m\"\u001b[39m  : alpha0\n\u001b[0;32m      4\u001b[0m     ,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigma0\u001b[39m\u001b[38;5;124m\"\u001b[39m  : sigma0\n\u001b[0;32m      5\u001b[0m     ,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m : nEpochs\n\u001b[0;32m      6\u001b[0m     }\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m alpha0       \u001b[38;5;129;01min\u001b[39;00m \u001b[43malphas\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sigma0       \u001b[38;5;129;01min\u001b[39;00m sigmas\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m nEpochs      \u001b[38;5;129;01min\u001b[39;00m epochs\n\u001b[0;32m     10\u001b[0m ]\n\u001b[0;32m     12\u001b[0m header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(som_params[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaximum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMedian\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStd. Deviation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m df_ds \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'alphas' is not defined"
     ]
    }
   ],
   "source": [
    "som_params = [\n",
    "    {\n",
    "     \"alpha0\"  : alpha0\n",
    "    ,\"sigma0\"  : sigma0\n",
    "    ,\"nEpochs\" : nEpochs\n",
    "    }\n",
    "    for alpha0       in alphas\n",
    "    for sigma0       in sigmas\n",
    "    for nEpochs      in epochs\n",
    "]\n",
    "\n",
    "header = list(som_params[0].keys()) + ['Minimum', 'Maximum', 'Median', 'Mean', 'Std. Deviation']\n",
    "\n",
    "df_ds = {}\n",
    "for dataset_name in datasets: # For this specific dataset\n",
    "    print(dataset_name)\n",
    "    df = df_results.loc[df_results['dataset_name'] == dataset_name] # get simulation results\n",
    "\n",
    "    count = 0\n",
    "    df_data   = np.zeros((len(som_params), len(header))) # matriz que guardará resultados numéricos\n",
    "    for params in som_params:\n",
    "        df_case = df.loc[(df['alpha0']  == params['alpha0']) & \n",
    "                         (df['sigma0']  == params['sigma0']) &\n",
    "                         (df['nEpochs'] == params['nEpochs'])]\n",
    "\n",
    "        # converting confusion matrix from string to numpy array\n",
    "        cm_ts = np.array([[int(x) for x in result[1:-1].split()] for result in df_case['cm_ts'].values])\n",
    "\n",
    "        #data = cm_ts\n",
    "        length = cm_ts.shape[1]\n",
    "        cm_side = int(np.sqrt(length))\n",
    "\n",
    "        acc   = [0]*len(cm_ts)\n",
    "        for i in range(len(cm_ts)):\n",
    "            cm = np.reshape(cm_ts[i], (cm_side,cm_side))\n",
    "            acc[i] = np.trace(cm)/np.sum(cm)\n",
    "\n",
    "        df_data[count,:] = np.matrix([\n",
    "            params['alpha0'], params['sigma0'], params['nEpochs'],\n",
    "            min(acc), max(acc), np.median(acc), np.mean(acc), np.std(acc)])\n",
    "        count+=1\n",
    "\n",
    "\n",
    "    df_ds[dataset_name] = pd.DataFrame(df_data, columns=header)\n",
    "    print(df_ds[dataset_name].head()) # TODO: display\n",
    "    print('-'*100,'\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the best values by higher mean in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m,:]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf_ds\u001b[49m\u001b[38;5;241m.\u001b[39mvalues()])\n\u001b[0;32m      2\u001b[0m idx_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df_ds\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m      3\u001b[0m df_rols \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mheader, index\u001b[38;5;241m=\u001b[39m[idx_label])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_ds' is not defined"
     ]
    }
   ],
   "source": [
    "data = np.array([df.sort_values('Mean', ascending=False).iloc[0,:].values for df in df_ds.values()])\n",
    "idx_label = list(df_ds.keys())\n",
    "df_rols = pd.DataFrame(data, columns=header, index=[idx_label])\n",
    "df_rols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globlal OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of random_states: 100\n",
      "# of datasets: 6\n",
      "\n",
      "# of cases: 600\n"
     ]
    }
   ],
   "source": [
    "# constant hyperparameters:\n",
    "test_size = 0.2\n",
    "scaleType = 'min-max'\n",
    "n_resamplings = 100\n",
    "\n",
    "# vector of random states for train/test split\n",
    "random_states = np.random.randint(np.iinfo(np.int32).max, size=n_resamplings).tolist()\n",
    "cases = [\n",
    "    {\n",
    "         \"dataset_name\" : dataset_name\n",
    "        ,\"random_state\":  random_state\n",
    "    }\n",
    "    for dataset_name in datasets.keys()\n",
    "    for random_state in random_states\n",
    "]\n",
    "\n",
    "print(\"# of random_states: {}\\n# of datasets: {}\\n\".format(\n",
    "    len(random_states), len(list(datasets.keys()))))\n",
    "\n",
    "print(\"# of cases: {}\".format(len(cases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "def evalGOLS(case):\n",
    "    dataset_name = case['dataset_name']\n",
    "    random_state = case['random_state']\n",
    "    \n",
    "    X = datasets[dataset_name]['features'].values\n",
    "    Y = datasets[dataset_name]['labels'].values\n",
    "    \n",
    "    # train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # scaling features\n",
    "    X_tr_norm, X_ts_norm = scale_feat(X_train, X_test, scaleType='min-max')\n",
    "\n",
    "    model = linear_model.LinearRegression().fit(X_tr_norm,y_train)\n",
    "    \n",
    "    # Evaluating in the train set\n",
    "    y_tr_pred = model.predict(X_tr_norm)\n",
    "    y_tr_pred = np.round(np.clip(y_tr_pred, 0, 1)) # rounding prediction numbers\n",
    "\n",
    "    cm_tr = confusion_matrix(dummie2multilabel(y_train),\n",
    "                             dummie2multilabel(y_tr_pred)\n",
    "                            ).flatten() # matrix => array\n",
    "    \n",
    "    # Evaluating in the test set\n",
    "    y_ts_pred = model.predict(X_ts_norm)\n",
    "    y_ts_pred = np.round(np.clip(y_ts_pred, 0, 1)) # rounding prediction numbers\n",
    "\n",
    "    cm_ts = confusion_matrix(dummie2multilabel(y_test),\n",
    "                             dummie2multilabel(y_ts_pred)\n",
    "                            ).flatten() # matrix => array\n",
    "\n",
    "    \n",
    "    data = [dataset_name, random_state]+[cm_tr]+[cm_ts]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                               | 0/600 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "\n",
    "data = [None]*len(cases)\n",
    "\n",
    "pool = Pool()\n",
    "data =[result for result in tqdm.tqdm(pool.imap_unordered(evalGOLS,cases), total=len(cases))]\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "results = np.vstack(data)\n",
    "header  = [\"dataset_name\", \"random_state\", \"cm_tr\", \"cm_ts\"]\n",
    "results_df = pd.DataFrame(results, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing results (taking the best values by higher mean in accuracy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "header = ['Minimum', 'Maximum', 'Median', 'Mean', 'Std. Deviation']\n",
    "\n",
    "data      = np.zeros(( len(datasets.keys()), len(header) ))\n",
    "idx_label = [' ']*len(datasets.keys())\n",
    "count=0\n",
    "for dataset_name in datasets: # For this specific dataset\n",
    "    df = results_df.loc[results_df['dataset_name'] == dataset_name] # get simulation results\n",
    "    \n",
    "    # converting confusion matrices to numpy matrix\n",
    "    cm_ts = np.array([array for array in df['cm_ts'].values])\n",
    "       \n",
    "    length = cm_ts.shape[1]\n",
    "    cm_side = int(np.sqrt(length))\n",
    "    acc   = [0]*len(cm_ts)\n",
    "    for i in range(len(cm_ts)):\n",
    "        cm = np.reshape(cm_ts[i], (cm_side,cm_side))\n",
    "        acc[i] = np.trace(cm)/np.sum(cm)\n",
    "\n",
    "    data[count,:] = np.array([min(acc), max(acc), np.median(acc), np.mean(acc), np.std(acc)])\n",
    "    idx_label[count] = dataset_name\n",
    "    count+=1\n",
    "    \n",
    "df_ols = pd.DataFrame(data, columns=header, index=[idx_label])\n",
    "df_ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Dataset', 'Model']+list(df_ols.columns)\n",
    "\n",
    "temp_rols = df_rols.rename_axis('Dataset').reset_index().loc[:,[x for x in header if x!='Model']]\n",
    "temp_rols.insert(1,'Model',['ROLS']*len(datasets.keys()))\n",
    "\n",
    "temp_ols = df_ols.rename_axis('Dataset').reset_index()\n",
    "temp_ols.insert(1,'Model',['OLS']*len(datasets.keys()))\n",
    "\n",
    "print(\n",
    "    pd.concat([temp_ols,temp_rols]).sort_index()\n",
    ") # TODO: display"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
