{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Classification by regional modeling</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification by regional modeling consists in a five-step approach:\n",
    "1. Setting the hyper-parameters. In this step, we specify the number of SOM prototypes $C$. It must be also defined as\n",
    "the maximum number of regions $K_{max}$. Without any prior knowledge, we will set in this example $K_{max} = \\sqrt{C}$.\n",
    "\n",
    "\n",
    "2. SOM training. In order to build regional models, follow the procedure introduced by Vesanto and Alhoniemi [1].\n",
    "Thus, the very first step requires training the SOM as usual, with $C$ prototypes.\n",
    "\n",
    "\n",
    "3. Clustering of the SOM. The step consists in performing clustering over the $C$ SOM prototypes. Although one may\n",
    "use any clustering algorithm for this step, for the sake of simplicity, we use the standard K-means algorithm in\n",
    "combination with the Davies–Bouldin (DB) index. The DB index is a clustering validity measure commonly used for\n",
    "finding the optimal number of clusters, but any suitable measure can be equally used (see [2]). Thus, we compute\n",
    "$K = 1, 2, ... K_{max}$ partitioning of the SOM prototypes and the corresponding DB index value as well.\n",
    "The optimal partitioning, represented by $K_{opt}$ partitions, is then the value of $K$ wich minimizes the DB index.\n",
    "\n",
    "\n",
    "4. Partitioning SOM prototypes into regions. Once $K_{opt}$ is selected, the $r$-th cluster of SOM prototypes,\n",
    "$r = 1...K_{opt}$, is composed of all weight vectors $w_i$ that are mapped onto the prototype $p_r$ of the K-means\n",
    "algorithm. More formally, the set of SOM prototypes associated with the r-th prototype of the K-means algorithm\n",
    "is defined as: $$W_r = \\{w_i \\in R^{p+q} | \\|w_i-p_r\\| < \\|w_i-p_j\\|, \\forall j =1,...,K_{opt}, j\\neq r \\}$$\n",
    "\n",
    "\n",
    "5. Mapping data points to regions. The fourth step consists in finding $K_{opt}$ data partitions, denoted by\n",
    "$\\{X_1\\}$, $\\{X_2\\}$, ... , $\\{X_{K_{opt}}\\}$ of the training dataset by mapping each datapoint to a region\n",
    "$r \\in \\{1, ... , K_{opt}\\}$. In other words, let us denote $N_r$ as the number of data vectors in $\\{X_r\\}$.\n",
    "Then, the partition $\\{X_r\\}$ is composed of those input vectors $x_{rμ}$, $μ = 1, ... , N_r$ , whose closest SOM\n",
    "prototype belongs to $W_r$.\n",
    "\n",
    "\n",
    "6. Building classification models over the regions. Finally, once the original dataset has been divided into $K_{opt}$\n",
    "subsets (one per region), the last step consists in building $K_{opt}$ regional classification models using\n",
    "$X_r$, $r = 1, ... , K_{opt}$.\n",
    "\n",
    "* Vertebral Column\n",
    "* Wall-Following\n",
    "* Alzheimer\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "[1] J. Vesanto, E. Alhoniemi, Clustering of the self-organizing map, IEEE Trans.\n",
    "Neural Netw. 11 (2000) 586–600.\n",
    "\n",
    "[2] M. Halkidi, Y. Batistakis, M. Vazirgiannis, On clustering validation techniques, J. Intell. Inf. Syst. 17 (2001) 107–145."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 719 ms\n",
      "Wall time: 176 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from math import ceil\n",
    "from load_dataset import get_datasets\n",
    "\n",
    "from devcode.utils import dummie2multilabel, scale_feat\n",
    "from devcode.models.som import SOM\n",
    "from devcode.models.regional_learning import RegionalModel\n",
    "from devcode import run_round\n",
    "\n",
    "from devcode.utils.metrics import DB\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "datasets    = get_datasets()\n",
    "dataset_name='pk'\n",
    "\n",
    "X = datasets[dataset_name]['features'].values\n",
    "Y = datasets[dataset_name]['labels'].values\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "# Train/Test split = 80%/20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=test_size)\n",
    "\n",
    "# scaling features\n",
    "X_tr_norm, X_ts_norm = scale_feat(X_train, X_test, scaleType='min-max')\n",
    "\n",
    "#N = len(dataset['features'].index) # number of datapoints\n",
    "N = len(X_tr_norm) # number of datapoints in the train split\n",
    "l = ceil((5*N**.5)**.5) # side length of square grid of neurons\n",
    "\n",
    "som = SOM(l,l)\n",
    "som_params={\n",
    "    'alpha0':    0.01,\n",
    "    'sigma0':    1,\n",
    "    'nEpochs':   1,\n",
    "    'verboses':  0            \n",
    "}\n",
    "\n",
    "C = l**2 # number of SOM neurons in the 2D grid\n",
    "k_values = [i for i in range(2, ceil(np.sqrt(C)))] # 2 to sqrt(C)\n",
    "cluster_params={\n",
    "    'n_clusters': {'metric':   DB,        # when a dictionary is pass a search begins\n",
    "                   'criteria': np.argmin, # search for smallest DB score \n",
    "                   'k_values': k_values}, # around the values provided in 'k_values'\n",
    "    'n_init':     10, # number of initializations\n",
    "    'init':       'random', \n",
    "    #'n_jobs':     -1\n",
    "}\n",
    "\n",
    "linearModel = linear_model.LinearRegression(n_jobs=-1)\n",
    "\n",
    "rm = RegionalModel(som, linearModel)\n",
    "rm.fit(X=X_tr_norm, Y=y_train, verboses=0,\n",
    "        SOM_params     = som_params,\n",
    "        Cluster_params = cluster_params)\n",
    "\n",
    "# Evaluating in the test dataset\n",
    "y_pred = rm.predict(X_ts_norm)\n",
    "y_pred = np.round(np.clip(y_pred, 0, 1)) # rounding prediction numbers\n",
    "\n",
    "cm = confusion_matrix(dummie2multilabel(y_test),\n",
    "                      dummie2multilabel(y_pred))\n",
    "#cm = np.asarray(cm).reshape(-1) # matrix => array\n",
    "acc=0\n",
    "total=sum(sum(cm))\n",
    "for j in range(len(cm)):\n",
    "    acc += cm[j,j] # summing the diagonal\n",
    "acc/=total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8205128205128205"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
