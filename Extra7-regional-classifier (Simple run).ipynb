{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Classification by regional modeling</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification by regional modeling consists in a five-step approach:\n",
    "1. Setting the hyper-parameters. In this step, we specify the number of SOM prototypes $C$. It must be also defined as\n",
    "the maximum number of regions $K_{max}$. Without any prior knowledge, we will set in this example $K_{max} = \\sqrt{C}$.\n",
    "\n",
    "\n",
    "2. SOM training. In order to build regional models, follow the procedure introduced by Vesanto and Alhoniemi [1].\n",
    "Thus, the very first step requires training the SOM as usual, with $C$ prototypes.\n",
    "\n",
    "\n",
    "3. Clustering of the SOM. The step consists in performing clustering over the $C$ SOM prototypes. Although one may\n",
    "use any clustering algorithm for this step, for the sake of simplicity, we use the standard K-means algorithm in\n",
    "combination with the Davies–Bouldin (DB) index. The DB index is a clustering validity measure commonly used for\n",
    "finding the optimal number of clusters, but any suitable measure can be equally used (see [2]). Thus, we compute\n",
    "$K = 1, 2, ... K_{max}$ partitioning of the SOM prototypes and the corresponding DB index value as well.\n",
    "The optimal partitioning, represented by $K_{opt}$ partitions, is then the value of $K$ wich minimizes the DB index.\n",
    "\n",
    "\n",
    "4. Partitioning SOM prototypes into regions. Once $K_{opt}$ is selected, the $r$-th cluster of SOM prototypes,\n",
    "$r = 1...K_{opt}$, is composed of all weight vectors $w_i$ that are mapped onto the prototype $p_r$ of the K-means\n",
    "algorithm. More formally, the set of SOM prototypes associated with the r-th prototype of the K-means algorithm\n",
    "is defined as: $$W_r = \\{w_i \\in R^{p+q} | \\|w_i-p_r\\| < \\|w_i-p_j\\|, \\forall j =1,...,K_{opt}, j\\neq r \\}$$\n",
    "\n",
    "\n",
    "5. Mapping data points to regions. The fourth step consists in finding $K_{opt}$ data partitions, denoted by\n",
    "$\\{X_1\\}$, $\\{X_2\\}$, ... , $\\{X_{K_{opt}}\\}$ of the training dataset by mapping each datapoint to a region\n",
    "$r \\in \\{1, ... , K_{opt}\\}$. In other words, let us denote $N_r$ as the number of data vectors in $\\{X_r\\}$.\n",
    "Then, the partition $\\{X_r\\}$ is composed of those input vectors $x_{rμ}$, $μ = 1, ... , N_r$ , whose closest SOM\n",
    "prototype belongs to $W_r$.\n",
    "\n",
    "\n",
    "6. Building classification models over the regions. Finally, once the original dataset has been divided into $K_{opt}$\n",
    "subsets (one per region), the last step consists in building $K_{opt}$ regional classification models using\n",
    "$X_r$, $r = 1, ... , K_{opt}$.\n",
    "\n",
    "* Vertebral Column\n",
    "* Wall-Following\n",
    "* Alzheimer\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "[1] J. Vesanto, E. Alhoniemi, Clustering of the self-organizing map, IEEE Trans.\n",
    "Neural Netw. 11 (2000) 586–600.\n",
    "\n",
    "[2] M. Halkidi, Y. Batistakis, M. Vazirgiannis, On clustering validation techniques, J. Intell. Inf. Syst. 17 (2001) 107–145."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 766 ms\n",
      "Wall time: 144 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from math import ceil\n",
    "from load_dataset import datasets\n",
    "\n",
    "from devcode.utils import dummie2multilabel, scale_feat\n",
    "from devcode.models.som import SOM\n",
    "from devcode.models.regional_learning import RegionalModel\n",
    "from devcode import run_round\n",
    "\n",
    "from devcode.utils.metrics import DB\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "dataset_name='pk'\n",
    "\n",
    "X = datasets[dataset_name]['features'].values\n",
    "Y = datasets[dataset_name]['labels'].values\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "# Train/Test split = 80%/20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=test_size)\n",
    "\n",
    "# scaling features\n",
    "X_tr_norm, X_ts_norm = scale_feat(X_train, X_test, scaleType='min-max')\n",
    "\n",
    "#N = len(dataset['features'].index) # number of datapoints\n",
    "N = len(X_tr_norm) # number of datapoints in the train split\n",
    "l = ceil((5*N**.5)**.5) # side length of square grid of neurons\n",
    "\n",
    "som = SOM(l,l)\n",
    "som_params={\n",
    "    'alpha0':    0.01,\n",
    "    'sigma0':    1,\n",
    "    'nEpochs':   1,\n",
    "    'verboses':  0            \n",
    "}\n",
    "\n",
    "C = l**2 # number of SOM neurons in the 2D grid\n",
    "k_values = [i for i in range(2, ceil(np.sqrt(C)))] # 2 to sqrt(C)\n",
    "cluster_params={\n",
    "    'n_clusters': {'metric':   DB,        # when a dictionary is pass a search begins\n",
    "                   'criteria': np.argmin, # search for smallest DB score \n",
    "                   'k_values': k_values}, # around the values provided in 'k_values'\n",
    "    'n_init':     10, # number of initializations\n",
    "    'init':       'random', \n",
    "    #'n_jobs':     -1\n",
    "}\n",
    "\n",
    "linearModel = linear_model.LinearRegression(n_jobs=-1)\n",
    "\n",
    "rm = RegionalModel(som, linearModel)\n",
    "rm.fit(X=X_tr_norm, Y=y_train, verboses=0,\n",
    "        SOM_params     = som_params,\n",
    "        Cluster_params = cluster_params)\n",
    "\n",
    "# Evaluating in the test dataset\n",
    "y_pred = rm.predict(X_ts_norm)\n",
    "y_pred = np.round(np.clip(y_pred, 0, 1)) # rounding prediction numbers\n",
    "\n",
    "cm = confusion_matrix(dummie2multilabel(y_test),\n",
    "                      dummie2multilabel(y_pred))\n",
    "#cm = np.asarray(cm).reshape(-1) # matrix => array\n",
    "acc=0\n",
    "total=sum(sum(cm))\n",
    "for j in range(len(cm)):\n",
    "    acc += cm[j,j] # summing the diagonal\n",
    "acc/=total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230769230769231"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'labels_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_round\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRegionalModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSOM_class\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModel_class\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinearModel\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_rounded\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Portfolio\\Research\\regional-classifiers\\devcode\\__init__.py:45\u001b[0m, in \u001b[0;36mrun_round\u001b[1;34m(X, y, test_size, method, tr_params, is_rounded)\u001b[0m\n\u001b[0;32m     42\u001b[0m X_tr_norm, X_ts_norm \u001b[38;5;241m=\u001b[39m scale_feat(X_train, X_test, scaleType\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin-max\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtr_params)\n\u001b[1;32m---> 45\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m y_pred_tr \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_tr_norm)\n\u001b[0;32m     48\u001b[0m y_pred_ts \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_ts_norm)\n",
      "File \u001b[1;32mD:\\Portfolio\\Research\\regional-classifiers\\devcode\\models\\regional_learning.py:67\u001b[0m, in \u001b[0;36mRegionalModel.fit\u001b[1;34m(self, X, Y, verboses, SOM_params, Cluster_params, Model_params)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCluster \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mk_opt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSOM\u001b[38;5;241m.\u001b[39mneurons)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Model training\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregion_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregionalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# finding labels of datapoints\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verboses \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart of Model training at \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()))\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregional_models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m k_opt\n",
      "File \u001b[1;32mD:\\Portfolio\\Research\\regional-classifiers\\devcode\\models\\regional_learning.py:93\u001b[0m, in \u001b[0;36mRegionalModel.regionalize\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)):  \u001b[38;5;66;03m# for each datapoint\u001b[39;00m\n\u001b[0;32m     91\u001b[0m     winner_som_idx, dist_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSOM\u001b[38;5;241m.\u001b[39mget_winner(\n\u001b[0;32m     92\u001b[0m         X[i], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dist_matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# find closest neuron\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     region \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCluster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels_\u001b[49m[winner_som_idx]  \u001b[38;5;66;03m# find neuron label index in kmeans\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# if the region don't have a model is because it didn't have datapoints in the train set\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m region \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mempty_regions:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'labels_'"
     ]
    }
   ],
   "source": [
    "run_round(X, Y, test_size, RegionalModel, {\"SOM_class\": som, \"Model_class\": linearModel}, is_rounded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of regional OLS in the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphas: [0.1, 0.30000000000000004, 0.5]\n",
      "sigmas: [3.0, 6.5, 10.0]\n",
      "epochs: [100, 300, 500]\n",
      "\n",
      "# of alphas: 3\n",
      "# of sigmas: 3\n",
      "# of epochs: 3\n",
      "# of random_states: 100\n",
      "# of datasets: 6\n",
      "\n",
      "# of cases: 16200\n"
     ]
    }
   ],
   "source": [
    "# constant hyperparameters:\n",
    "test_size = 0.2\n",
    "scaleType = 'min-max'\n",
    "n_resamplings = 100\n",
    "\n",
    "# hyperparameters grid search:\n",
    "num = 3\n",
    "alphas = np.linspace(0.1, 0.5,  num=num).tolist()\n",
    "sigmas = np.linspace(3,    10,   num=num).tolist()\n",
    "epochs = np.linspace(100,  500, num=num, dtype='int').tolist()\n",
    "\n",
    "# vector of random states for train/test split\n",
    "random_states = np.random.randint(np.iinfo(np.int32).max, size=n_resamplings).tolist()\n",
    "cases = [\n",
    "    {\n",
    "         \"dataset_name\" : dataset_name\n",
    "        ,\"random_state\":  random_state\n",
    "        ,\"som_params\":    { \"alpha0\"  : alpha0\n",
    "                           ,\"sigma0\"  : sigma0\n",
    "                           ,\"nEpochs\" : nEpochs\n",
    "                          }\n",
    "    }\n",
    "    # hyperparameters possible values\n",
    "    for dataset_name in datasets.keys()\n",
    "    for random_state in random_states\n",
    "    for alpha0       in alphas\n",
    "    for sigma0       in sigmas\n",
    "    for nEpochs      in epochs\n",
    "]\n",
    "\n",
    "print(\"alphas: {}\\nsigmas: {}\\nepochs: {}\\n\".format(alphas,sigmas,epochs))\n",
    "\n",
    "print(\"# of alphas: {}\\n# of sigmas: {}\\n# of epochs: {}\\n# of random_states: {}\\n# of datasets: {}\\n\".format(\n",
    "    len(alphas), len(sigmas), len(epochs), len(random_states), len(list(datasets.keys()))))\n",
    "\n",
    "print(\"# of cases: {}\".format(len(cases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def evalRLM(case):\n",
    "    dataset_name = case['dataset_name']\n",
    "    random_state = case['random_state']\n",
    "    som_params   = case['som_params']\n",
    "    \n",
    "    X = datasets[dataset_name]['features'].values\n",
    "    Y = datasets[dataset_name]['labels'].values\n",
    "    \n",
    "    # Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=test_size, random_state=random_state)\n",
    "    # scaling features\n",
    "    X_tr_norm, X_ts_norm = scale_feat(X_train, X_test, scaleType=scaleType)\n",
    "\n",
    "    N = len(X_tr_norm) # number of datapoints in the train split\n",
    "    l = ceil((5*N**.5)**.5) # side length of square grid of neurons\n",
    "\n",
    "    som = SOM(l,l)\n",
    "\n",
    "    C = l**2 # number of SOM neurons in the 2D grid\n",
    "    k_values = [i for i in range(2, ceil(np.sqrt(C)))] # 2 to sqrt(C)\n",
    "    cluster_params={\n",
    "        'n_clusters': {'metric':   DB,        # when a dictionary is pass a search begins\n",
    "                       'criteria': np.argmin, # search for smallest DB score \n",
    "                       'k_values': k_values}, # around the values provided in 'k_values'\n",
    "        'n_init':     10, # number of initializations\n",
    "        'init':       'random'\n",
    "        #'n_jobs':     0\n",
    "    }\n",
    "\n",
    "    linearModel = linear_model.LinearRegression()\n",
    "\n",
    "    rlm = RegionalModel(som, linearModel)\n",
    "    rlm.fit(X=X_tr_norm, Y=y_train,\n",
    "            SOM_params     = som_params,\n",
    "            Cluster_params = cluster_params)\n",
    "\n",
    "    # Evaluating in the train set\n",
    "    y_tr_pred = rlm.predict(X_tr_norm)\n",
    "    y_tr_pred = np.round(np.clip(y_tr_pred, 0, 1)) # rounding prediction numbers\n",
    "\n",
    "    cm_tr = confusion_matrix(dummie2multilabel(y_train),\n",
    "                             dummie2multilabel(y_tr_pred)\n",
    "                            ).reshape(-1) # matrix => array\n",
    "\n",
    "    # Evaluating in the test set\n",
    "    y_ts_pred = rlm.predict(X_ts_norm)\n",
    "    y_ts_pred = np.round(np.clip(y_ts_pred, 0, 1)) # rounding prediction numbers\n",
    "\n",
    "    cm_ts = confusion_matrix(dummie2multilabel(y_test),\n",
    "                             dummie2multilabel(y_ts_pred)\n",
    "                            ).reshape(-1) # matrix => array\n",
    "\n",
    "    data = [dataset_name, random_state]+list(som_params.values())+[cm_tr]+[cm_ts]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "#cm_size = (Y.shape[1]+1) # number of rows/columns in the confusion matrix\n",
    "#header = ['set']+list(cases[0].keys())+['C({},{})'.format(i,j) for i in range(cm_size) \n",
    "#                                                               for j in range(cm_size)]\n",
    "\n",
    "pool = Pool() # Create a multiprocessing Pool\n",
    "data = pool.map(evalRLM, cases[0:2]) # process data_inputs iterable with pool\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "results = np.vstack(data)\n",
    "header  = [\"dataset_name\", \"random_state\", \"alpha0\", \"sigma0\", \"nEpochs\", \"cm_tr\", \"cm_ts\"]\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=header)\n",
    "\n",
    "filename = \"ROLS - all - n_res={n_resamplings} - {datetime}.csv\".format(\n",
    "    n_resamplings=n_resamplings,\n",
    "    datetime=datetime.datetime.now()\n",
    ")\n",
    "results_df.to_csv(filename,index=False) # saving results in csv file\n",
    "\n",
    "\n",
    "# 2019-03-13 19:22:45.669104\n",
    "# 2019-03-18 01:43:55.780175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browser notification when cell finishs with '%%notify'\n",
    "# import jupyternotify\n",
    "# ip = get_ipython()\n",
    "# ip.register_magics(jupyternotify.JupyterNotifyMagics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%notify` not found.\n"
     ]
    }
   ],
   "source": [
    "%%notify\n",
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "\n",
    "data = [None]*len(cases)\n",
    "count=0\n",
    "pool = Pool()\n",
    "for i in tqdm.tqdm(pool.imap_unordered(evalRLM, cases), total=len(cases)):\n",
    "    data[count] = i\n",
    "    count+=1\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "\n",
    "results = np.vstack(data)\n",
    "header  = [\"dataset_name\", \"random_state\", \"alpha0\", \"sigma0\", \"nEpochs\", \"cm_tr\", \"cm_ts\"]\n",
    "results_df = pd.DataFrame(results, columns=header)\n",
    "\n",
    "filename = \"ROLS - all - n_res={n_resamplings} - {datetime}.csv\".format(\n",
    "    n_resamplings=n_resamplings,\n",
    "    datetime=datetime.datetime.now()\n",
    ")\n",
    "results_df.to_csv(filename,index=False) # saving results in csv file\n",
    "\n",
    "# [{elapsed}<{remaining}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Processing results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>random_state</th>\n",
       "      <th>alpha0</th>\n",
       "      <th>sigma0</th>\n",
       "      <th>nEpochs</th>\n",
       "      <th>cm_tr</th>\n",
       "      <th>cm_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vc2c</td>\n",
       "      <td>127815836</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100</td>\n",
       "      <td>[156  13  15  64]</td>\n",
       "      <td>[32  9  7 14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vc2c</td>\n",
       "      <td>127815836</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100</td>\n",
       "      <td>[153  16  16  63]</td>\n",
       "      <td>[32  9  6 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vc2c</td>\n",
       "      <td>127815836</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>100</td>\n",
       "      <td>[162   7  21  58]</td>\n",
       "      <td>[34  7  9 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vc2c</td>\n",
       "      <td>127815836</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>300</td>\n",
       "      <td>[157  12  21  58]</td>\n",
       "      <td>[32  9  8 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vc2c</td>\n",
       "      <td>127815836</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>300</td>\n",
       "      <td>[155  14  19  60]</td>\n",
       "      <td>[33  8  7 14]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset_name  random_state  alpha0  sigma0  nEpochs              cm_tr  \\\n",
       "0         vc2c     127815836     0.1     3.0      100  [156  13  15  64]   \n",
       "1         vc2c     127815836     0.1    10.0      100  [153  16  16  63]   \n",
       "2         vc2c     127815836     0.1     6.5      100  [162   7  21  58]   \n",
       "3         vc2c     127815836     0.1    10.0      300  [157  12  21  58]   \n",
       "4         vc2c     127815836     0.1     6.5      300  [155  14  19  60]   \n",
       "\n",
       "           cm_ts  \n",
       "0  [32  9  7 14]  \n",
       "1  [32  9  6 15]  \n",
       "2  [34  7  9 12]  \n",
       "3  [32  9  8 13]  \n",
       "4  [33  8  7 14]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading simulation results\n",
    "df_results = pd.read_csv('results/regional-results/ROLS - all - n_res=100 - 2019-07-10.csv')\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for each data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vc2c\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m cm_ts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;28mint\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m result[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m df_case[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcm_ts\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#data = cm_ts\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m length \u001b[38;5;241m=\u001b[39m \u001b[43mcm_ts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     31\u001b[0m cm_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39msqrt(length))\n\u001b[0;32m     33\u001b[0m acc   \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(cm_ts)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "som_params = [\n",
    "    {\n",
    "     \"alpha0\"  : alpha0\n",
    "    ,\"sigma0\"  : sigma0\n",
    "    ,\"nEpochs\" : nEpochs\n",
    "    }\n",
    "    for alpha0       in alphas\n",
    "    for sigma0       in sigmas\n",
    "    for nEpochs      in epochs\n",
    "]\n",
    "\n",
    "header = list(som_params[0].keys()) + ['Minimum', 'Maximum', 'Median', 'Mean', 'Std. Deviation']\n",
    "\n",
    "df_ds = {}\n",
    "for dataset_name in datasets: # For this specific dataset\n",
    "    print(dataset_name)\n",
    "    df = df_results.loc[df_results['dataset_name'] == dataset_name] # get simulation results\n",
    "\n",
    "    count = 0\n",
    "    df_data   = np.zeros((len(som_params), len(header))) # matriz que guardará resultados numéricos\n",
    "    for params in som_params:\n",
    "        df_case = df.loc[(df['alpha0']  == params['alpha0']) & \n",
    "                         (df['sigma0']  == params['sigma0']) &\n",
    "                         (df['nEpochs'] == params['nEpochs'])]\n",
    "\n",
    "        # converting confusion matrix from string to numpy array\n",
    "        cm_ts = np.array([[int(x) for x in result[1:-1].split()] for result in df_case['cm_ts'].values])\n",
    "\n",
    "        #data = cm_ts\n",
    "        length = cm_ts.shape[1]\n",
    "        cm_side = int(np.sqrt(length))\n",
    "\n",
    "        acc   = [0]*len(cm_ts)\n",
    "        for i in range(len(cm_ts)):\n",
    "            cm = np.reshape(cm_ts[i], (cm_side,cm_side))\n",
    "            acc[i] = np.trace(cm)/np.sum(cm)\n",
    "\n",
    "        df_data[count,:] = np.matrix([\n",
    "            params['alpha0'], params['sigma0'], params['nEpochs'],\n",
    "            min(acc), max(acc), np.median(acc), np.mean(acc), np.std(acc)])\n",
    "        count+=1\n",
    "\n",
    "\n",
    "    df_ds[dataset_name] = pd.DataFrame(df_data, columns=header)\n",
    "    print(df_ds[dataset_name].head()) # TODO: display\n",
    "    print('-'*100,'\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the best values by higher mean in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Empty data passed with indices specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m,:]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m df_ds\u001b[38;5;241m.\u001b[39mvalues()])\n\u001b[0;32m      2\u001b[0m idx_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df_ds\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m----> 3\u001b[0m df_rols \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43midx_label\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df_rols\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:694\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    684\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    685\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    686\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    692\u001b[0m         )\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:351\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# _prep_ndarray ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    347\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    348\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    349\u001b[0m )\n\u001b[1;32m--> 351\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:418\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;129;01mor\u001b[39;00m values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;66;03m# Could let this raise in Block constructor, but we get a more\u001b[39;00m\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m#  helpful exception message this way.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty data passed with indices specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    420\u001b[0m     passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    421\u001b[0m     implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n",
      "\u001b[1;31mValueError\u001b[0m: Empty data passed with indices specified."
     ]
    }
   ],
   "source": [
    "data = np.array([df.sort_values('Mean', ascending=False).iloc[0,:].values for df in df_ds.values()])\n",
    "idx_label = list(df_ds.keys())\n",
    "df_rols = pd.DataFrame(data, columns=header, index=[idx_label])\n",
    "df_rols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globlal OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant hyperparameters:\n",
    "test_size = 0.2\n",
    "scaleType = 'min-max'\n",
    "n_resamplings = 100\n",
    "\n",
    "# vector of random states for train/test split\n",
    "random_states = np.random.randint(np.iinfo(np.int32).max, size=n_resamplings).tolist()\n",
    "cases = [\n",
    "    {\n",
    "         \"dataset_name\" : dataset_name\n",
    "        ,\"random_state\":  random_state\n",
    "    }\n",
    "    for dataset_name in datasets.keys()\n",
    "    for random_state in random_states\n",
    "]\n",
    "\n",
    "print(\"# of random_states: {}\\n# of datasets: {}\\n\".format(\n",
    "    len(random_states), len(list(datasets.keys()))))\n",
    "\n",
    "print(\"# of cases: {}\".format(len(cases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "def evalGOLS(case):\n",
    "    dataset_name = case['dataset_name']\n",
    "    random_state = case['random_state']\n",
    "    \n",
    "    X = datasets[dataset_name]['features'].values\n",
    "    Y = datasets[dataset_name]['labels'].values\n",
    "    \n",
    "    # train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # scaling features\n",
    "    X_tr_norm, X_ts_norm = scale_feat(X_train, X_test, scaleType='min-max')\n",
    "\n",
    "    model = linear_model.LinearRegression().fit(X_tr_norm,y_train)\n",
    "    \n",
    "    # Evaluating in the train set\n",
    "    y_tr_pred = model.predict(X_tr_norm)\n",
    "    y_tr_pred = np.round(np.clip(y_tr_pred, 0, 1)) # rounding prediction numbers\n",
    "\n",
    "    cm_tr = confusion_matrix(dummie2multilabel(y_train),\n",
    "                             dummie2multilabel(y_tr_pred)\n",
    "                            ).flatten() # matrix => array\n",
    "    \n",
    "    # Evaluating in the test set\n",
    "    y_ts_pred = model.predict(X_ts_norm)\n",
    "    y_ts_pred = np.round(np.clip(y_ts_pred, 0, 1)) # rounding prediction numbers\n",
    "\n",
    "    cm_ts = confusion_matrix(dummie2multilabel(y_test),\n",
    "                             dummie2multilabel(y_ts_pred)\n",
    "                            ).flatten() # matrix => array\n",
    "\n",
    "    \n",
    "    data = [dataset_name, random_state]+[cm_tr]+[cm_ts]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "\n",
    "data = [None]*len(cases)\n",
    "\n",
    "pool = Pool()\n",
    "data =[result for result in tqdm.tqdm(pool.imap_unordered(evalGOLS,cases), total=len(cases))]\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "results = np.vstack(data)\n",
    "header  = [\"dataset_name\", \"random_state\", \"cm_tr\", \"cm_ts\"]\n",
    "results_df = pd.DataFrame(results, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresults_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing results (taking the best values by higher mean in accuracy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name \u001b[38;5;129;01min\u001b[39;00m datasets: \u001b[38;5;66;03m# For this specific dataset\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mresults_df\u001b[49m\u001b[38;5;241m.\u001b[39mloc[results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m dataset_name] \u001b[38;5;66;03m# get simulation results\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# converting confusion matrices to numpy matrix\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     cm_ts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([array \u001b[38;5;28;01mfor\u001b[39;00m array \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcm_ts\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "header = ['Minimum', 'Maximum', 'Median', 'Mean', 'Std. Deviation']\n",
    "\n",
    "data      = np.zeros(( len(datasets.keys()), len(header) ))\n",
    "idx_label = [' ']*len(datasets.keys())\n",
    "count=0\n",
    "for dataset_name in datasets: # For this specific dataset\n",
    "    df = results_df.loc[results_df['dataset_name'] == dataset_name] # get simulation results\n",
    "    \n",
    "    # converting confusion matrices to numpy matrix\n",
    "    cm_ts = np.array([array for array in df['cm_ts'].values])\n",
    "       \n",
    "    length = cm_ts.shape[1]\n",
    "    cm_side = int(np.sqrt(length))\n",
    "    acc   = [0]*len(cm_ts)\n",
    "    for i in range(len(cm_ts)):\n",
    "        cm = np.reshape(cm_ts[i], (cm_side,cm_side))\n",
    "        acc[i] = np.trace(cm)/np.sum(cm)\n",
    "\n",
    "    data[count,:] = np.array([min(acc), max(acc), np.median(acc), np.mean(acc), np.std(acc)])\n",
    "    idx_label[count] = dataset_name\n",
    "    count+=1\n",
    "    \n",
    "df_ols = pd.DataFrame(data, columns=header, index=[idx_label])\n",
    "df_ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m header \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[43mdf_ols\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      3\u001b[0m temp_rols \u001b[38;5;241m=\u001b[39m df_rols\u001b[38;5;241m.\u001b[39mrename_axis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39mloc[:,[x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m header \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      4\u001b[0m temp_rols\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m,[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROLS\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(datasets\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_ols' is not defined"
     ]
    }
   ],
   "source": [
    "header = ['Dataset', 'Model']+list(df_ols.columns)\n",
    "\n",
    "temp_rols = df_rols.rename_axis('Dataset').reset_index().loc[:,[x for x in header if x!='Model']]\n",
    "temp_rols.insert(1,'Model',['ROLS']*len(datasets.keys()))\n",
    "\n",
    "temp_ols = df_ols.rename_axis('Dataset').reset_index()\n",
    "temp_ols.insert(1,'Model',['OLS']*len(datasets.keys()))\n",
    "\n",
    "print(\n",
    "    pd.concat([temp_ols,temp_rols]).sort_index()\n",
    ") # TODO: display"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
