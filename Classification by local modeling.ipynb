{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Classification by local modeling</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "1. [Introduction](#introduction)\n",
    "\n",
    "2. [Local Ordinary Least Squares (L-OLS)](#lols)\n",
    "    \n",
    "    2.1. [Influence of the number of clusters on model accuracy](#lols-#-clusters)\n",
    "    \n",
    "3. [Local Least Squares Support Vector Machine (L-LSSVM)](#l_lssvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction <a class=\"anchor\" id=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic classification by local modeling is a two-step approach for modeling:\n",
    "\n",
    "1. An unsupervised clustering algorithm is run to find regions in the dataset;\n",
    "2. For each region, a model is built with the respective data partition.\n",
    "\n",
    "For inference the procedure is similar:\n",
    "\n",
    "1. A similarity metric is used to determine the new data point region, e.g. euclidian distance from regions prototypes;\n",
    "2. The model from that specific region is used to predict the class of the new data point.\n",
    "\n",
    "There are a lot of clustering algorithms but, for the sake of simplicity, it will be used only K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%run -i \"load_dataset.py\" # loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class **LocalModel**, implemented below, create an easy way to implement and test local models for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasModel:\n",
    "    \"\"\"Class the implements a dummy model in case of homogenous region\"\"\"\n",
    "    \n",
    "    def __init__(self, class_label):\n",
    "        self.class_label = class_label\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.vstack( [self.class_label]*len(X) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "\n",
    "class LocalModel:\n",
    "    \"\"\"Class of Local Models.\"\"\"\n",
    "    \n",
    "    def __init__(self, ClusterAlg, ModelAlg):\n",
    "        self.ClusterAlg = ClusterAlg\n",
    "        self.ModelAlg = ModelAlg\n",
    "        \n",
    "        self.clusters = None\n",
    "        self.models   = None\n",
    "        self.targets_dim_ = None\n",
    "        \n",
    "        self.empty_regions = []\n",
    "        \n",
    "\n",
    "    def fit(self, X, Y, verboses=0):\n",
    "        self.targets_dim_ = Y.shape[1] # dimension of target values\n",
    "        \n",
    "        # Clustering\n",
    "        if verboses==1: print(\"Start of clusterization: {}\".format(datetime.datetime.now()))\n",
    "        \n",
    "        self.ClusterAlg.fit(X)\n",
    "        self.clusters = self.ClusterAlg.cluster_centers_ \n",
    "                \n",
    "        # Local models training\n",
    "        if verboses==1: print(\"Start of local models training: {}\".format(datetime.datetime.now()))\n",
    "                \n",
    "        n_clusters  = self.ClusterAlg.n_clusters\n",
    "        labels      = self.ClusterAlg.labels_ # labels of each datapoints\n",
    "        self.models = [{}]*n_clusters\n",
    "        for i in range(n_clusters): # for each region\n",
    "            Xi = X[np.where(labels == i)[0]]\n",
    "            Yi = Y[np.where(labels == i)[0]]\n",
    "            \n",
    "            model_i = None\n",
    "            local_classes = np.unique(Yi, axis=0) # list of local classes\n",
    "            if len(Xi) == 0: # empty region\n",
    "                self.empty_regions.append(i)\n",
    "                \n",
    "            elif len(local_classes) == 1: # homogenous region, only one class\n",
    "                model_i = BiasModel(local_classes)\n",
    "                \n",
    "            else: # region not empty or homogeneous\n",
    "                self.ModelAlg.fit(Xi,Yi)\n",
    "                model_i = copy(self.ModelAlg)\n",
    "                            \n",
    "            self.models[i] = model_i\n",
    "            \n",
    "            \n",
    "    def predict(self, X, rounded=False):\n",
    "        predictions = np.zeros((\n",
    "            len(X),           # number of samples\n",
    "            self.targets_dim_ # size of output Y\n",
    "        ))\n",
    "        \n",
    "        region_list = self.ClusterAlg.predict(X)#.astype(int) # list of regions of each sample\n",
    "        for i in range(len(X)):\n",
    "            predictions[i,:] = self.models[region_list[i]].predict(X[i].reshape(1, -1))\n",
    "            # should I worry about regions with no models? (empty regions in training)\n",
    "            # isn't that a sign of bad clustering?\n",
    "        \n",
    "        return predictions if not rounded else np.round(np.clip(predictions, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complementary functions to process the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Function to scale data\n",
    "def scale_feat(X_train, X_test, scaleType='min-max'):\n",
    "    if scaleType=='min-max' or scaleType=='std':\n",
    "        X_tr_norm = np.copy(X_train) # making a copy to leave original available\n",
    "        X_ts_norm = np.copy(X_test)\n",
    "        scaler = MinMaxScaler() if scaleType=='min-max' else StandardScaler()\n",
    "        scaler.fit(X_tr_norm)\n",
    "        X_tr_norm = scaler.transform(X_tr_norm)\n",
    "        X_ts_norm = scaler.transform(X_ts_norm)\n",
    "        return (X_tr_norm, X_ts_norm)\n",
    "    else:\n",
    "        raise ValueError(\"Scale type not defined. Use 'min-max' or 'std' instead.\")\n",
    "\n",
    "# convert dummies to multilabel\n",
    "def dummie2multilabel(X):\n",
    "    N = len(X)\n",
    "    X_multi = np.zeros((N,1),dtype='int')\n",
    "    for i in range(N):\n",
    "        temp = np.where(X[i]==1)[0] # find where 1 is found in the array\n",
    "        if temp.size == 0: # is an empty array, there is no '1' in the X[i] array\n",
    "            X_multi[i] = 0 # so we denote this class '0'\n",
    "        else:\n",
    "            X_multi[i] = temp[0] + 1 # we have +1 because 0 denote the class with an empty array\n",
    "    return X_multi.T[0]\n",
    "\n",
    "# takes confusion matrix and evaluate the accuracy\n",
    "def cm2acc(cm):\n",
    "    acc=0\n",
    "    total=sum(sum(cm))\n",
    "    for j in range(cm.shape[0]):\n",
    "        acc += cm[j,j] # summing the diagonal\n",
    "    acc/=total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Local Ordinary Least Squares (L-OLS) <a class=\"anchor\" id=\"lols\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of `LocalModel` class running with OLS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-8fb6bd354a1d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mdataset_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"vc2c\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[0mX\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mdataset_name\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'features'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[0mY\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mdataset_name\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'labels'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "dataset_name = \"vc2c\"\n",
    "X = datasets[dataset_name]['features'].values\n",
    "Y = datasets[dataset_name]['labels'].values\n",
    "\n",
    "# Train/Test split = 80%/20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2)\n",
    "# scaling features\n",
    "X_tr_norm, X_ts_norm = scale_feat(X_train, X_test, scaleType='min-max')\n",
    "\n",
    "n_clusters=5\n",
    "print(\"Number of clusters: {}\".format(n_clusters))\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=10, n_jobs=-1, random_state=0)\n",
    "linReg = LinearRegression(n_jobs=-1)\n",
    "\n",
    "lm = LocalModel(ClusterAlg=kmeans, ModelAlg=linReg)\n",
    "lm.fit(X_tr_norm, y_train, verboses=1)\n",
    "\n",
    "y_pred_tr = lm.predict(X_tr_norm, rounded=True)\n",
    "y_pred_ts = lm.predict(X_ts_norm, rounded=True)\n",
    "\n",
    "\n",
    "cm_tr = confusion_matrix(dummie2multilabel(y_train),\n",
    "                         dummie2multilabel(y_pred_tr))\n",
    "cm_ts = confusion_matrix(dummie2multilabel(y_test),\n",
    "                         dummie2multilabel(y_pred_ts))\n",
    "   \n",
    "acc_tr = cm2acc(cm_tr)\n",
    "acc_ts = cm2acc(cm_ts)\n",
    "\n",
    "print(\"Train accuracy: {}\\nTest accuracy:  {}\".format(acc_tr, acc_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Influence of the number of clusters on model accuracy <a class=\"anchor\" id=\"lols-#-clusters\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline, we will use a global model, in this case, a global linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters:\n",
    "n_init = 100 # number of independent runs\n",
    "test_size = 0.2 # test size of 20%\n",
    "scaleType = 'min-max' # type of feature scaling\n",
    "\n",
    "results_df = {'GOLS': {}, 'LOLS': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<timed exec>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Applying Global Ordinary Least Squares\n",
    "header = [dataset_name for dataset_name in datasets.keys()]\n",
    "results = np.zeros((n_init, len(header)))\n",
    "\n",
    "count=0 # counting datasets\n",
    "for dataset_name in datasets:\n",
    "    X = datasets[dataset_name]['features'].values\n",
    "    Y = datasets[dataset_name]['labels'].values\n",
    "    \n",
    "    acc = [0]*n_init\n",
    "    for i in range(n_init):\n",
    "        # Train/Test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=test_size)\n",
    "        \n",
    "        # scaling features\n",
    "        X_tr_norm, X_ts_norm = scale_feat(X_train, X_test, scaleType=scaleType)\n",
    "        \n",
    "        linearModel = linear_model.LinearRegression(n_jobs=-1).fit(X_tr_norm,y_train)\n",
    "                   \n",
    "        # Evaluating in the test dataset\n",
    "        y_pred = linearModel.predict(X_ts_norm)\n",
    "        y_pred = np.round(np.clip(y_pred, 0, 1)) # rounding prediction numbers\n",
    "\n",
    "        cm = confusion_matrix(dummie2multilabel(y_test),\n",
    "                              dummie2multilabel(y_pred))\n",
    "        acc[i] = cm2acc(cm)\n",
    "            \n",
    "    results[:,count] = acc\n",
    "    count+=1\n",
    "    \n",
    "results_gols = pd.DataFrame(results, columns=header)\n",
    "# Wall time: 3.21 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<timed exec>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import datetime\n",
    "\n",
    "# Applying Local Ordinary Least Squares\n",
    "\n",
    "n_ks = 10 # number of clusters to evaluate for each dataset\n",
    "# for each dataset\n",
    "for dataset_name in datasets:\n",
    "    print(\"Starting '{}' at {}\".format(dataset_name,datetime.datetime.now()))\n",
    "    \n",
    "    X = datasets[dataset_name]['features'].values\n",
    "    Y = datasets[dataset_name]['labels'].values\n",
    "    \n",
    "    max_k = int(0.8*len(X)*(1-test_size)) # k_max = 80% of the number of samples on the train set\n",
    "    ks = np.linspace(2, max_k, num=n_ks, dtype='int')\n",
    "    print(\"ks = {}\".format(ks))\n",
    "    header = [\"k={}\".format(i) for i in ks] # header with the number of clusters\n",
    "    \n",
    "    results = np.zeros((n_init, len(ks)))\n",
    "    for j in range(len(ks)): # for each value of k\n",
    "        print(ks[j])\n",
    "        for i in range(n_init): # run n_init independent train/test split and evaluation\n",
    "            # Train/Test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=test_size)\n",
    "\n",
    "            # scaling features\n",
    "            X_tr_norm, X_ts_norm = scale_feat(X_train, X_test, scaleType=scaleType)\n",
    "    \n",
    "            # creating clustering and model algorithm\n",
    "            kmeans = KMeans(n_clusters=ks[j], n_init=10, n_jobs=-1)\n",
    "            linReg = LinearRegression(n_jobs=-1)\n",
    "            \n",
    "            # creating and fitting local model\n",
    "            lm = LocalModel(ClusterAlg=kmeans, ModelAlg=linReg)\n",
    "            lm.fit(X_tr_norm, y_train)\n",
    "\n",
    "            # evaluating accuracy on the test set\n",
    "            y_pred_ts = lm.predict(X_ts_norm, rounded=True)\n",
    "            cm_ts = confusion_matrix(dummie2multilabel(y_test),\n",
    "                                     dummie2multilabel(y_pred_ts))\n",
    "\n",
    "            acc_ts = cm2acc(cm_ts)\n",
    "            results[i,j] = acc_ts\n",
    "\n",
    "        \n",
    "    results_df = pd.DataFrame(results, columns=header)\n",
    "    filename = \"LOLS - {} - n_init {} - {}\".format(dataset_name, n_init, datetime.datetime.now())\n",
    "    results_df.to_csv(filename, sep='\\t') # saving results in CSV file\n",
    "    print(\"{} done!\".format(dataset_name))\n",
    "    print(\" \")\n",
    "    \n",
    "# CPU times: user 55min 5s, sys: 18min 22s, total: 1h 13min 28s\n",
    "# Wall time: 12h 57min 42s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Processing results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_gols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-10-b24f3c75a7cc>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mresults\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;34m'GOLS'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'LOLS'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mresults\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'GOLS'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mresults_gols\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m results['LOLS']['pk'] = pd.read_csv('LOLS - pk - n_init 100 - 2019-06-12 09:49:35.319083',\n",
      "\u001B[1;31mNameError\u001B[0m: name 'results_gols' is not defined"
     ]
    }
   ],
   "source": [
    "#loading results\n",
    "results = {'GOLS': {}, 'LOLS': {}}\n",
    "\n",
    "results['GOLS'] = results_gols\n",
    "\n",
    "results['LOLS']['pk'] = pd.read_csv('LOLS - pk - n_init 100 - 2019-06-12 09:49:35.319083',\n",
    "                                    delim_whitespace=True)\n",
    "results['LOLS']['vc2c'] = pd.read_csv('LOLS - vc2c - n_init 100 - 2019-06-11 20:54:03.314362',\n",
    "                                      delim_whitespace=True)\n",
    "results['LOLS']['vc3c'] = pd.read_csv('LOLS - vc3c - n_init 100 - 2019-06-11 20:56:11.824959',\n",
    "                                      delim_whitespace=True)\n",
    "results['LOLS']['wf2f'] = pd.read_csv('LOLS - wf2f - n_init 100 - 2019-06-12 09:48:11.911241',\n",
    "                                      delim_whitespace=True)\n",
    "results['LOLS']['wf4f'] = pd.read_csv('LOLS - wf4f - n_init 100 - 2019-06-12 07:23:53.153429',\n",
    "                                      delim_whitespace=True)\n",
    "results['LOLS']['wf24f'] = pd.read_csv('LOLS - wf24f - n_init 100 - 2019-06-12 04:48:26.751605',\n",
    "                                       delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-11-2d1d5746b73b>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# py.init_notebook_mode(connected=True) # enabling plot within jupyter notebook\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[1;32mfor\u001B[0m \u001B[0mdataset_name\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m     \u001B[0mks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mresults\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'LOLS'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mdataset_name\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[0mks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minsert\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m\"k=0\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "# py.init_notebook_mode(connected=True) # enabling plot within jupyter notebook\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    ks = results['LOLS'][dataset_name].columns.tolist()\n",
    "    ks.insert(0,\"k=0\")\n",
    "    \n",
    "    data = [{}]*(len(ks)+1)\n",
    "    data[0] = go.Box(\n",
    "        y=results['GOLS'][dataset_name].values,\n",
    "        name = ks[0][2:],\n",
    "        marker = dict(color = '#2980b9')\n",
    "    )\n",
    "    for i in range(2, len(ks)):\n",
    "        trace = go.Box(\n",
    "            y=results['LOLS'][dataset_name][ks[i]].values,\n",
    "            name = ks[i][2:],\n",
    "            marker = dict(color = '#2980b9')\n",
    "        )\n",
    "        data[i] = trace\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title = \"Accuracy vs number of clusters [{}]\".format(dataset_name),\n",
    "        showlegend=False,\n",
    "        yaxis=dict(title=\"Accuracy on the test set\"),\n",
    "        xaxis=dict(title=\"Number of clusters\")\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data,layout=layout)\n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that:\n",
    "\n",
    "* In the Vertebral Column dataset we had a drop in accuracy when using local modeling, showing us that the problem is simple enough to be resolved with a linear classifier;\n",
    "* In the Wall-following dataset we had an improvement in accuracy, more features we had more difference we saw. That gives us evidence that the classification problem has a non-linear decision boundary and that local modeling had the ability to approximate this non-linearity by a combination of local linear classifiers;\n",
    "* In the Parkinson dataset we had a slight improvement in accuracy, showing us that local linear classifier was better than global linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Local Least Squares Support Vector Machine (L-LSSVM) <a class=\"anchor\" id=\"l_lssvm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of `LocalModel` class running with LSSVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<timed exec>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# %autoreload\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from lssvm import LSSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# small_datasets =['vc2c', 'vc3c', 'pk']\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    print(dataset_name)\n",
    "\n",
    "    X = datasets[dataset_name]['features'].values\n",
    "    Y = datasets[dataset_name]['labels'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2)  # Train/Test split = 80%/20%\n",
    "    X_tr_norm, X_ts_norm = scale_feat(X_train, X_test, scaleType='min-max') # scaling features\n",
    "    \n",
    "    k_values = np.linspace(2, np.ceil(np.sqrt(len(X_train))), num=5, dtype='int').tolist() # 2 to sqrt(N)\n",
    "    \n",
    "    for n_clusters in k_values:\n",
    "        print(\"# of clusters: {}\".format(n_clusters))\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=0)\n",
    "\n",
    "        clf_dict = {\n",
    "            'linear': LSSVM(gamma=1, kernel='linear'),\n",
    "            'poly'  : LSSVM(gamma=1, kernel='poly', d=2),\n",
    "            'rbf'   : LSSVM(gamma=1, kernel='rbf', sigma=1)\n",
    "        }\n",
    "\n",
    "        for kernel_type, clf in clf_dict.items():\n",
    "            print('kernel: {}'.format(kernel_type))\n",
    "            lm = LocalModel(ClusterAlg=kmeans, ModelAlg=clf)\n",
    "            lm.fit(X_tr_norm, y_train, verboses=0)\n",
    "\n",
    "            y_pred_tr = lm.predict(X_tr_norm)\n",
    "            y_pred_ts = lm.predict(X_ts_norm)\n",
    "\n",
    "            cm_tr = confusion_matrix(dummie2multilabel(y_train),\n",
    "                                     dummie2multilabel(y_pred_tr))\n",
    "            cm_ts = confusion_matrix(dummie2multilabel(y_test),\n",
    "                                     dummie2multilabel(y_pred_ts))\n",
    "\n",
    "            acc_tr = cm2acc(cm_tr)\n",
    "            acc_ts = cm2acc(cm_ts)\n",
    "\n",
    "            print(\"Train accuracy: {}\\nTest accuracy:  {}\\n\".format(acc_tr, acc_ts))\n",
    "\n",
    "    print('\\n')\n",
    "    print('#'*60)\n",
    "    print('\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}