{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Regional Least Squares Support Vector Machine (R-LSSVM)</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "1. [Methodology](#methodology)\n",
    "\n",
    "\n",
    "2. [Simulations](#simulations)\n",
    "    \n",
    "    2.1 [Regioanl LSSVM](#r-lssvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Methodology <a class=\"anchor\" id=\"methodology\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach was:\n",
    "\n",
    "1. For 50 times:\n",
    "    \n",
    "    1.1 Divide the data set between train/test in stratified manner;\n",
    "    \n",
    "    1.2 Used 5-fold stratified cross-validation on the training set to choose best hyperparameters;\n",
    "    \n",
    "    1.3 Fit model in the whole train set with best hyperparameters;\n",
    "    \n",
    "    1.3 Make predictions in test set;\n",
    "    \n",
    "2. Distribution of the performance metric on train and test sets was evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simulations <a class=\"anchor\" id=\"simulations\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  Features.shape:   # of classes:\n",
      "vc2c      (310, 6)          2\n",
      "vc3c      (310, 6)          3\n",
      "wf24f     (5456, 24)        4\n",
      "wf4f      (5456, 4)         4\n",
      "wf2f      (5456, 2)         4\n",
      "pk        (195, 22)         2\n"
     ]
    }
   ],
   "source": [
    "%run -i \"load_dataset.py\" # loading dataset\n",
    "%run -i \"aux_func.py\"     # loading auxilary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes confusion matrix and evaluate the accuracy\n",
    "def cm2acc(cm):\n",
    "    acc=0\n",
    "    total=sum(sum(cm))\n",
    "    for j in range(cm.shape[0]):\n",
    "        acc += cm[j,j] # summing the diagonal\n",
    "    acc/=total\n",
    "    return acc\n",
    "\n",
    "# convert dummies to multilabel\n",
    "def dummie2multilabel(X):\n",
    "    N = len(X)\n",
    "    X_multi = np.zeros((N,1),dtype='int')\n",
    "    for i in range(N):\n",
    "        temp = np.where(X[i]==1)[0] # find where 1 is found in the array\n",
    "        if temp.size == 0: # is an empty array, there is no '1' in the X[i] array\n",
    "            X_multi[i] = 0 # so we denote this class '0'\n",
    "        else:\n",
    "            X_multi[i] = temp[0] + 1 # we have +1 because 0 denote the class with an empty array\n",
    "    return X_multi.T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Regional LSSVM <a class=\"anchor\" id=\"r-lssvm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating `RegionalModel` class below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.cluster import KMeans\n",
    "from copy import copy\n",
    "\n",
    "class BiasModel:\n",
    "    'Class the implements a dummy model in case of homogenous region.'\n",
    "    \n",
    "    def __init__(self, class_label):\n",
    "        self.class_label = class_label\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.vstack( [self.class_label]*len(X) )\n",
    "    \n",
    "\n",
    "class RegionalModel:\n",
    "    'Class of Regional Models.'\n",
    "    \n",
    "    def __init__(self, SOM_class, Model_class, Cluster_class=None):\n",
    "        self.SOM             = SOM_class\n",
    "        self.Cluster         = Cluster_class\n",
    "        self.Model           = Model_class\n",
    "        self.region_labels   = []\n",
    "        self.regional_models = []\n",
    "        self.empty_regions   = []\n",
    "        self.targets_dim_    = None\n",
    "        \n",
    "\n",
    "    def fit(self, X, Y, verboses=0, SOM_params=None, Cluster_params=None, Model_params=None):\n",
    "        self.targets_dim_ = Y.shape[1] # dimension of target values\n",
    "        \n",
    "        # SOM training\n",
    "        if SOM_params is not None:\n",
    "            if verboses==1: print(\"Start of SOM training at {}\".format(datetime.datetime.now()))\n",
    "            self.SOM.fit(X=X, **SOM_params)\n",
    "        \n",
    "        # Cluster training\n",
    "        if Cluster_params is not None:\n",
    "            if verboses==1: print(\"Start of clustering SOM prototypes at {}\".format(datetime.datetime.now()))\n",
    "            \n",
    "            # Search for k_opt\n",
    "            k_opt = None\n",
    "            if type(Cluster_params['n_clusters']) is dict: # a search is implied:\n",
    "                eval_function = Cluster_params['n_clusters']['metric']\n",
    "                find_best     = Cluster_params['n_clusters']['criteria']\n",
    "                k_values      = Cluster_params['n_clusters']['k_values']\n",
    "                \n",
    "                validation_index = [0]*len(k_values)\n",
    "                for i in range(len(k_values)):\n",
    "                    kmeans = KMeans(n_clusters=k_values[i],\n",
    "                                    n_init=10,\n",
    "                                    init='random'\n",
    "                                    #n_jobs=-1\n",
    "                                   ).fit(self.SOM.neurons)\n",
    "                    # test if number of distinct clusters == number of clusters specified\n",
    "                    centroids = kmeans.cluster_centers_\n",
    "                    if len(centroids) == len(np.unique(centroids,axis=0)):\n",
    "                        validation_index[i] = eval_function(kmeans,self.SOM.neurons)\n",
    "                    else:\n",
    "                        validation_index[i] = np.NaN\n",
    "                \n",
    "                k_opt = k_values[find_best(validation_index)]\n",
    "                if verboses==1: print(\"Best k found: {}\".format(k_opt))\n",
    "            else:\n",
    "                k_opt = Cluster_params['n_clusters']\n",
    "            \n",
    "            params = Cluster_params.copy()\n",
    "            del params['n_clusters'] # deleting unecessary param\n",
    "            # real training of clustering algorithm\n",
    "            self.Cluster = KMeans(n_clusters=k_opt, **params).fit(self.SOM.neurons)\n",
    "            \n",
    "        \n",
    "        # Model training\n",
    "        self.region_labels = self.regionalize(X) # finding labels of datapoints\n",
    "        if verboses==1: print(\"Start of Model training at {}\".format(datetime.datetime.now()))\n",
    "        \n",
    "        self.regional_models = [None]*k_opt    \n",
    "        for r in range(k_opt): # for each region\n",
    "            Xr = X[np.where(self.region_labels == r)[0]]\n",
    "            Yr = Y[np.where(self.region_labels == r)[0]]\n",
    "            \n",
    "            model_r = None\n",
    "            if len(Xr) == 0: # empty region\n",
    "                self.empty_regions.append(r)\n",
    "            \n",
    "            elif len( np.unique(Yr, axis=0) ) == 1: # homogenous region, only one class\n",
    "                model_r = BiasModel(np.unique(Yr, axis=0))\n",
    "                \n",
    "            else: # region not empty or homogeneous\n",
    "                self.Model.fit(Xr,Yr)\n",
    "                model_r = copy(self.Model)\n",
    "\n",
    "            self.regional_models[r] = model_r\n",
    "            \n",
    "                       \n",
    "                \n",
    "    def regionalize(self, X):\n",
    "        regions = np.zeros(len(X), dtype='int')\n",
    "        for i in range(len(X)): # for each datapoint\n",
    "            winner_som_idx, dist_matrix = self.SOM.get_winner(\n",
    "                                          X[i], dim=1, dist_matrix=True) # find closest neuron\n",
    "            region = self.Cluster.labels_[winner_som_idx] # find neuron label index in kmeans\n",
    "            \n",
    "            # if the region don't have a model is because it didn't have datapoints in the train set\n",
    "            if region in self.empty_regions: \n",
    "                dead_neurons, = np.where(self.Cluster.labels_==region)\n",
    "                dist_matrix[dead_neurons] = np.inf # taking off dead neurons from the play\n",
    "                \n",
    "                temp = np.argmin(dist_matrix)\n",
    "                region = self.Cluster.labels_[temp]\n",
    "                \n",
    "            regions[i] = region \n",
    "        \n",
    "        return regions\n",
    "          \n",
    "    \n",
    "    def predict(self, X):\n",
    "        # searching for a non-empty region\n",
    "        regions = [i for i in range(self.Cluster.n_clusters)]\n",
    "        not_empty_regions = list(set(regions) - set(self.empty_regions))\n",
    "        \n",
    "#         temp = self.regional_models[not_empty_regions[0]].intercept_\n",
    "        predictions = np.zeros((\n",
    "            len(X),           # number of samples\n",
    "            self.targets_dim_ # size of output Y\n",
    "        ))\n",
    "        \n",
    "        regions = self.regionalize(X)\n",
    "        for i in range(len(X)):\n",
    "            predictions[i,:] = self.regional_models[regions[i]].predict(X[i].reshape(1, -1))\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gammas = [1e-06, 0.0001, 0.01, 1.0, 100.0, 10000.0, 1000000.0]\n",
      "sigmas = [0.31622776601683794, 2.371373705661655, 17.78279410038923, 133.3521432163324, 1000.0]\n",
      "# of hps_cases = 35\n",
      " \n",
      "# of cases: 300\n"
     ]
    }
   ],
   "source": [
    "datasets_names = ['pk', 'vc2c', 'vc3c', 'wf2f', 'wf4f', 'wf24f']\n",
    "# constant hyperparameters:\n",
    "test_size = 0.5\n",
    "scaleType = 'min-max'\n",
    "n_init = 50 # number of independent runs\n",
    "\n",
    "\n",
    "# hyperparameters grid search:\n",
    "gammas = np.logspace(-6.0, 6.0, num=7).tolist()\n",
    "sigmas = np.logspace(-0.5, 3.0, num=5).tolist()\n",
    "\n",
    "print(\"gammas = {}\".format(gammas))\n",
    "print(\"sigmas = {}\".format(sigmas))\n",
    "\n",
    "hps_cases = [\n",
    "    { \"gamma\": gamma,\n",
    "      \"sigma\": sigma \n",
    "    }\n",
    "    for gamma in gammas\n",
    "    for sigma in sigmas\n",
    "]\n",
    "print(\"# of hps_cases = {}\".format(len(hps_cases)))\n",
    "\n",
    "# vector of random states for train/test split\n",
    "random_states = np.unique( \n",
    "    pd.read_csv('../Local_Modeling/simulation_results/G-LSSVM - n_init=50 - 2019-08-28 12:58:37.036007.csv',usecols=['random_state']).values\n",
    ").tolist()\n",
    "# random_states = np.random.randint(np.iinfo(np.int32).max, size=n_init).tolist()\n",
    "cases = [\n",
    "    {\n",
    "         \"dataset_name\": dataset_name\n",
    "        ,\"random_state\": random_state\n",
    "    }\n",
    "    # hyperparameters possible values\n",
    "    for dataset_name in datasets_names\n",
    "    for random_state in random_states\n",
    "]\n",
    "print(' ')\n",
    "print(\"# of cases: {}\".format(len(cases)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Índices Baseados em Critérios de Informação:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def IC_core(X, labels_pred):\n",
    "    unique_labels = np.unique(labels_pred)\n",
    "    N = len(X)\n",
    "    P = len(unique_labels)*X.shape[1]\n",
    "    \n",
    "    # Mean Squared Quantization Error\n",
    "    MSQE = 0\n",
    "    for label in unique_labels:\n",
    "        X_cluster = X[labels_pred==label]\n",
    "        cluster = np.mean(X_cluster, axis=0)\n",
    "        MSQE += np.sum((X_cluster - cluster)**2)\n",
    "    MSQE = (1/N)*MSQE\n",
    "           \n",
    "    lhs = N*log(MSQE/N) # left-hand side\n",
    "    \n",
    "    return (N, P, lhs)\n",
    "\n",
    "# Final Prediction Error\n",
    "def FPE(X, labels_pred):\n",
    "    N, P, lhs = IC_core(X, labels_pred)\n",
    "    if ((N+P)/(N-P))<=0: # not define in math the rhs\n",
    "        return np.inf\n",
    "    else:\n",
    "        return lhs + N*log((N+P)/(N-P))\n",
    "\n",
    "# Akaike Information Criteria\n",
    "def AIC(X, labels_pred):\n",
    "    N, P, lhs = IC_core(X, labels_pred)\n",
    "    return lhs + 2*P\n",
    "\n",
    "# Bayesian Information Criteria\n",
    "def BIC(X, labels_pred):\n",
    "    N, P, lhs = IC_core(X, labels_pred)\n",
    "    return lhs + P*log(N)\n",
    "\n",
    "# Minimum Description Length\n",
    "def MDL(X, labels_pred):\n",
    "    N, P, lhs = IC_core(X, labels_pred)\n",
    "    return lhs + (P/2)*log(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of cluster validation metrics: 8\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import base\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/clustering.html\n",
    "cluster_val_metrics = [\n",
    "#     {\n",
    "#         'name'    : 'Adjusted Rand Index',\n",
    "#         'f'       : metrics.adjusted_rand_score,\n",
    "#         'get_best': np.argmax,\n",
    "#         'type'    : 'supervised'\n",
    "#     },\n",
    "#     {\n",
    "#         'name'    : 'Adjusted Mutual Information',\n",
    "#         'f'       : metrics.adjusted_mutual_info_score,\n",
    "#         'get_best': np.argmax,\n",
    "#         'type'    : 'supervised'\n",
    "#     },\n",
    "#     {\n",
    "#         'name'    : 'V-measure',\n",
    "#         'f'       : metrics.v_measure_score,\n",
    "#         'get_best': np.argmax,\n",
    "#         'type'    : 'supervised'\n",
    "#     },\n",
    "#     {\n",
    "#         'name'    : 'Fowlkes-Mallows',\n",
    "#         'f'       : metrics.fowlkes_mallows_score,\n",
    "#         'get_best': np.argmax,\n",
    "#         'type'    : 'supervised'\n",
    "#     },\n",
    "    {\n",
    "        'name'    : 'Silhouette',\n",
    "        'f'       : metrics.silhouette_score,\n",
    "        'get_best': np.argmax,\n",
    "        'type'    : 'unsupervised'\n",
    "    },\n",
    "    {\n",
    "        'name'    : 'Calinski-Harabasz',\n",
    "        'f'       : metrics.calinski_harabasz_score,\n",
    "        'get_best': np.argmax,\n",
    "        'type'    : 'unsupervised'\n",
    "    },\n",
    "    {\n",
    "        'name'    : 'Davies-Bouldin',\n",
    "        'f'       : metrics.davies_bouldin_score,\n",
    "        'get_best': np.argmin,\n",
    "        'type'    : 'unsupervised'\n",
    "    },\n",
    "    {\n",
    "        'name'    : 'Dunn',\n",
    "        'f'       : base.dunn_fast,\n",
    "        'get_best': np.argmax,\n",
    "        'type'    : 'unsupervised'\n",
    "    },\n",
    "    {\n",
    "        'name'    : 'Final Prediction Error',\n",
    "        'f'       : FPE,\n",
    "        'get_best': np.argmin,\n",
    "        'type'    : 'unsupervised'\n",
    "    },\n",
    "    {\n",
    "        'name'    : 'Akaike Information Criteria',\n",
    "        'f'       : AIC,\n",
    "        'get_best': np.argmin,\n",
    "        'type'    : 'unsupervised'\n",
    "    },\n",
    "    {\n",
    "        'name'    : 'Bayesian Information Criteria',\n",
    "        'f'       : BIC,\n",
    "        'get_best': np.argmin,\n",
    "        'type'    : 'unsupervised'\n",
    "    },\n",
    "    {\n",
    "        'name'    : 'Minimum Description Length',\n",
    "        'f'       : MDL,\n",
    "        'get_best': np.argmin,\n",
    "        'type'    : 'unsupervised'\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"# of cluster validation metrics: {}\".format(len(cluster_val_metrics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções para avaliação dos agrupamentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parar com a porra dos warnings do sklearn\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "def eval_cluster(cluster_val_metrics, X, labels_true, labels_pred):\n",
    "    names = [metric['name'] for metric in cluster_val_metrics]\n",
    "    results = {}\n",
    "    \n",
    "    for metric in cluster_val_metrics:\n",
    "        if metric['type']=='supervised':\n",
    "            results[metric['name']] = metric['f'](labels_true, labels_pred)\n",
    "        else:\n",
    "            results[metric['name']] = metric['f'](X, labels_pred)\n",
    "            \n",
    "    return results\n",
    "    \n",
    "# eval_cluster(cluster_val_metrics, X_tr_norm, labels_true, labels_pred)\n",
    "\n",
    "def get_k_opt_suggestions(X, y, ks, cluster_val_metrics):\n",
    "    results = {metric['name']: [None]*len(ks) for metric in cluster_val_metrics}\n",
    "    for i in range(len(ks)):\n",
    "        kmeans = KMeans(n_clusters=ks[i], init='random').fit(X)\n",
    "        labels_true = y.ravel()\n",
    "        labels_pred = kmeans.labels_\n",
    "\n",
    "        temp = eval_cluster(cluster_val_metrics, X, labels_true, labels_pred)\n",
    "        for name in temp:\n",
    "            results[name][i] = temp[name]\n",
    "\n",
    "    suggestions = {metric['name']: np.nan for metric in cluster_val_metrics}\n",
    "    for i in range(len(cluster_val_metrics)):\n",
    "        metric = cluster_val_metrics[i]\n",
    "        suggestions[metric['name']]  = ks[metric['get_best'](results[metric['name']])]\n",
    "        \n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION TO EVAL R-LSSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lssvm import LSSVM\n",
    "from som import SOM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.cluster import KMeans\n",
    "from copy import copy\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Objective function in validation strategy\n",
    "def f_o(u):\n",
    "    return np.mean(u) - 2*np.std(u)\n",
    "\n",
    "# no SOM's hyperparameter optimization was done\n",
    "som_params={\n",
    "            'alpha0':    0.1,\n",
    "            'sigma0':    10,\n",
    "            'nEpochs':   300,\n",
    "            'verboses':  0            \n",
    "        }\n",
    "\n",
    "# preparando o cabeçalho:\n",
    "temp = [' ']*2*len(cluster_val_metrics)\n",
    "count=0\n",
    "for metric in cluster_val_metrics:\n",
    "    temp[count]   = \"$\\gamma_{opt}$ \"+\"[{}]\".format(metric['name'])\n",
    "    temp[count+1] = \"$\\sigma_{opt}$ \"+\"[{}]\".format(metric['name'])\n",
    "    count+=2\n",
    "# print(temp)\n",
    "header  = [\n",
    "    \"dataset_name\", \"random_state\",\n",
    "    \"# empty regions\", \"# homogeneous regions\"] + \\\n",
    "    [\"$\\gamma_{opt}$ [CV]\", \"$\\sigma_{opt}$ [CV]\"] + \\\n",
    "    temp +\\\n",
    "    [\"$k_{opt}$ [CV]\"] + \\\n",
    "    ['$k_{opt}$ '+'[{}]'.format(metric['name']) for metric in cluster_val_metrics] + \\\n",
    "    ['cv_score [{}]'.format(metric['name']) for metric in cluster_val_metrics] + \\\n",
    "    [\"eigenvalues\", \"eigenvalues_dtype\", \"cm_tr\", \"cm_ts\"]\n",
    "# cm_tr.dtype and cm_ts.dtype = 'int64'\n",
    "# print(header)\n",
    "\n",
    "\n",
    "# Regional Least-Squares Support Vector Machine\n",
    "# global optimization of hyperparameters\n",
    "def eval_RLSSVM(case):\n",
    "    filename = (\"./temp_rlssvm/results/R-LSSVM - {}.csv\".format(case)).replace(':','-')\n",
    "    my_file = Path(filename)\n",
    "    \n",
    "    if not my_file.is_file(): # compute if it doesn't exists\n",
    "        dataset_name = case['dataset_name']\n",
    "        random_state = case['random_state']\n",
    "\n",
    "        X = datasets[dataset_name]['features'].values\n",
    "        Y = datasets[dataset_name]['labels'].values\n",
    "\n",
    "        # Train/Test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,Y, stratify=np.unique(Y,axis=1), \n",
    "                                                            test_size=test_size, random_state=random_state)\n",
    "        # scaling features\n",
    "        X_tr_norm, X_ts_norm = scale_feat(X_train, X_test, scaleType=scaleType)\n",
    "\n",
    "        # solving multilabel problem in wall-following data set\n",
    "        y_temp = y_train\n",
    "        if y_train.ndim==2: \n",
    "            if y_train.shape[1] >= 2: y_temp = dummie2multilabel(y_train)\n",
    "                \n",
    "        # We first fit the SOM in the whole train set so we can get suggestion for number\n",
    "        # of clusters in K-Means\n",
    "        N = len(X_tr_norm)\n",
    "        l = int((5*N**.5)**.5) # size of square grid of neurons\n",
    "        som_tr = SOM(l,l)\n",
    "        som_tr.fit(X_tr_norm, **som_params)\n",
    "        \n",
    "        C  = l**2 # number of SOM neurons in the 2D grid\n",
    "        ks = np.arange( 2, int(C**(1/2))+1 ).tolist() # 2 to sqrt(C)\n",
    "        suggestions = get_k_opt_suggestions(som_tr.neurons, \n",
    "                                            np.empty(C), \n",
    "                                            ks, cluster_val_metrics)\n",
    "        unique_suggestions = np.unique(list(suggestions.values())).tolist()\n",
    "        \n",
    "        validation_scores = np.empty((len(unique_suggestions), 2)) # [k, cv_score] \n",
    "        best_hps_list     = [{}]*len(unique_suggestions)\n",
    "        count_v=0\n",
    "        for k in unique_suggestions: # para cada proposta de k_{opt}\n",
    "            # 5-fold stratified cross-validation for hyperparameter optimization\n",
    "            n_cases = len(hps_cases)\n",
    "            cv_scores = [0]*n_cases\n",
    "            for i in range(n_cases):\n",
    "                skf = StratifiedKFold(n_splits=5)\n",
    "                acc=[0]*5\n",
    "                count=0\n",
    "\n",
    "                # train/validation split\n",
    "                for tr_index, val_index in skf.split(X_tr_norm, y_temp): \n",
    "                    x_tr, x_val = X_tr_norm[tr_index], X_tr_norm[val_index]\n",
    "                    y_tr, y_val = y_train[tr_index],   y_train[val_index]\n",
    "\n",
    "                    # train the model on the train set\n",
    "                    N = len(x_tr)\n",
    "                    l = int((5*N**.5)**.5) # size of square grid of neurons\n",
    "                    som = SOM(l,l)\n",
    "                    cluster_params={'n_clusters': k, 'n_init': 10, 'init': 'random'}\n",
    "\n",
    "                    model_alg = LSSVM(kernel='rbf', **hps_cases[i])\n",
    "                    rm = RegionalModel(som, model_alg)\n",
    "                    rm.fit(X=x_tr, Y=y_tr, verboses=0,\n",
    "                           SOM_params     = som_params,\n",
    "                           Cluster_params = cluster_params)\n",
    "                    # eval model accuracy on validation set\n",
    "                    acc[count] = accuracy_score(y_val, rm.predict(x_val))\n",
    "                    count+=1\n",
    "                    \n",
    "                # apply objective function to cv accuracies\n",
    "                cv_scores[i] = f_o(acc)\n",
    "                \n",
    "            # the best hyperparameters are the ones that maximize the objective function\n",
    "            best_hps_list[count_v]       = hps_cases[ np.argmax(cv_scores) ]\n",
    "            validation_scores[count_v,:] = [k, np.amax(cv_scores)]\n",
    "            count_v+=1\n",
    "            \n",
    "        \n",
    "        # k_opt as the one with best cv_score and smallest value\n",
    "        best_k = int(validation_scores[validation_scores.argmax(axis=0)[1], 0])\n",
    "\n",
    "        # the best hyperparameters are the ones that maximize the objective function\n",
    "        best_hps = best_hps_list[validation_scores.argmax(axis=0)[1]]\n",
    "\n",
    "        # fit the model on best global hyperparameters\n",
    "        cluster_params={'n_clusters': best_k, 'n_init': 10, 'init': 'random'}\n",
    "        model_alg = LSSVM(kernel='rbf', **best_hps)\n",
    "            rm = RegionalModel(som_tr, model_alg)\n",
    "            rm.fit(X=X_tr_norm, Y=y_train, verboses=0,\n",
    "                   # SOM_params=som_params, # no need as the SOM was trained in the beginning\n",
    "                   Cluster_params = cluster_params)\n",
    "\n",
    "        # make predictions and evaluate model\n",
    "        y_pred_tr, y_pred_ts = rm.predict(X_tr_norm), rm.predict(X_ts_norm)\n",
    "        cm_tr = confusion_matrix(dummie2multilabel(y_train),\n",
    "                                 dummie2multilabel(y_pred_tr))\n",
    "        cm_ts = confusion_matrix(dummie2multilabel(y_test),\n",
    "                                 dummie2multilabel(y_pred_ts))\n",
    "\n",
    "        # getting eigenvalues of kernel matrices\n",
    "        n_lssvms=0 # counter for LSSVM models\n",
    "        models_idx = []\n",
    "        for i in range(len(rm.regional_models)):         \n",
    "            if isinstance(rm.regional_models[i], LSSVM):            \n",
    "                n_lssvms+=1\n",
    "                models_idx.append(i)\n",
    "    #     print(\"models_idx={}\".format(models_idx))\n",
    "\n",
    "        X_labels = rm.regionalize(X_tr_norm)\n",
    "        eigvals_list = [None, np.array([np.nan])]*n_lssvms\n",
    "        count=0\n",
    "        for i in models_idx:\n",
    "            x_region = X_tr_norm[X_labels==i]\n",
    "            K = rm.regional_models[i].kernel(x_region, x_region)\n",
    "            temp = np.linalg.eigvals(K)\n",
    "            eigvals_list[count] = temp#.tostring()\n",
    "            count+=2\n",
    "\n",
    "        eigvals = np.concatenate( eigvals_list, axis=0 )\n",
    "\n",
    "        k_opt                 = len(rm.regional_models)\n",
    "        n_empty_regions       = len(rm.empty_regions)\n",
    "        n_homogeneous_regions = 0\n",
    "        for i in range(len(rm.regional_models)):         \n",
    "            if isinstance(rm.regional_models[i], BiasModel):            \n",
    "                n_homogeneous_regions+=1\n",
    "\n",
    "        # Organizing suggestion of the cluster metrics\n",
    "        temp = [np.nan]*2*len(cluster_val_metrics)\n",
    "        count=0\n",
    "        for metric in cluster_val_metrics:\n",
    "            temp[count] = best_hps_list[\n",
    "                np.where( validation_scores[:,0]==suggestions[metric['name']] )[0][0]]['gamma']\n",
    "            temp[count+1] = best_hps_list[\n",
    "                np.where( validation_scores[:,0]==suggestions[metric['name']] )[0][0]]['sigma']\n",
    "            count+=2        \n",
    "        \n",
    "        data = np.array(\n",
    "            [dataset_name, random_state,\n",
    "             n_empty_regions, n_homogeneous_regions] + \\\n",
    "            [best_hps['gamma'], best_hps['sigma']] + \\\n",
    "            temp + \\\n",
    "            [k_opt] + \\\n",
    "            [value for value in list(suggestions.values())] + \\\n",
    "            [validation_scores[ \n",
    "                np.where( validation_scores[:,0]==suggestions[metric['name']] )[0][0] ,\n",
    "                1] for metric in cluster_val_metrics ] + \\\n",
    "            [eigvals.tostring(), eigvals.dtype, cm_tr.tostring(), cm_ts.tostring()]\n",
    "        ).reshape(1,-1)\n",
    "        \n",
    "        \n",
    "        results_df = pd.DataFrame(data, columns=header)\n",
    "        results_df.to_csv(filename,index=False) # saving results in csv file\n",
    "        \n",
    "        # saving clusters to .csv\n",
    "        kmeans_proto = rm.Cluster.cluster_centers_\n",
    "        neurons      = rm.SOM.neurons\n",
    "        \n",
    "        kmeans_filename  = (\"./temp_rlssvm/clusters/kmeans/L-LSSVM - {}.csv\".format(case)).replace(':','-')\n",
    "        neurons_filename = (\"./temp_rlssvm/clusters/som/L-LSSVM - {}.csv\".format(case)   ).replace(':','-')\n",
    "        \n",
    "        pd.DataFrame(kmeans_proto).to_csv(kmeans_filename, header=None, index=None)\n",
    "        pd.DataFrame(neurons).to_csv(neurons_filename, header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-13 19:56:15.743788\n",
      "CPU times: user 1.38 ms, sys: 0 ns, total: 1.38 ms\n",
      "Wall time: 1.38 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import datetime\n",
    "print(datetime.datetime.now())\n",
    "eval_RLSSVM(cases[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 1585.3min\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed: 1589.6min\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed: 1599.8min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed: 1608.4min\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed: 2832.9min\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed: 2833.1min\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed: 3220.2min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed: 3234.3min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 4396.3min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 4450.9min\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed: 4833.9min\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed: 4847.8min\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed: 5592.9min\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed: 6057.1min\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed: 6064.6min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed: 6470.8min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 7181.9min\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed: 7655.6min\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed: 7659.9min\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed: 7669.5min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed: 8811.5min\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed: 9236.7min\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed: 9242.9min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 9268.2min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed: 10450.8min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed: 10848.1min\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed: 10862.6min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed: 10886.3min\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed: 12055.5min\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed: 12059.5min\n",
      "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed: 12074.3min\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed: 12125.0min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 13650.9min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 13654.2min\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed: 13684.1min\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed: 13728.5min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed: 13958.6min\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed: 13958.6min\n",
      "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed: 13958.6min\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed: 13958.6min\n",
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed: 13958.6min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 13958.6min\n",
      "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed: 13958.6min\n",
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed: 13958.6min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/Mestrado/Pesquisa/Regional_Modeling/aux_func.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m data = Parallel(n_jobs=-1, verbose=51)( #6\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_RLSSVM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m~/Dropbox/Mestrado/Pesquisa/Regional_Modeling/venv/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Mestrado/Pesquisa/Regional_Modeling/venv/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Mestrado/Pesquisa/Regional_Modeling/venv/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# from random import shuffle\n",
    "# shuffle(cases) # better estimation of remaining time\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "data = Parallel(n_jobs=-1, verbose=51)( #6\n",
    "    delayed(eval_RLSSVM)(case) for case in reversed(cases)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see every column of pandas data frame on jupyter notebook\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "# display(results_df)\n",
    "\n",
    "import glob\n",
    "\n",
    "path = r'./temp_rlssvm/results' # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "df_results = pd.concat(li, axis=0, ignore_index=True)\n",
    "df_results\n",
    "# pd.read_csv(\"./temp/R-LSSVM - {'dataset_name': 'vc2c', 'random_state': 73470257, 'kernel': 'linear'}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get back the eigenvalues of each region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "[ 1.19462052e+02+0.00000000e+00j  1.01115026e+01+0.00000000e+00j\n",
      "  5.69025736e+00+0.00000000e+00j  2.89735797e+00+0.00000000e+00j\n",
      "  2.59291355e+00+0.00000000e+00j  1.58522926e+00+0.00000000e+00j\n",
      "  1.31880473e+00+0.00000000e+00j  9.71090509e-01+0.00000000e+00j\n",
      "  6.36440390e-01+0.00000000e+00j  4.61385429e-01+0.00000000e+00j\n",
      "  3.16833636e-01+0.00000000e+00j  1.05823181e-01+0.00000000e+00j\n",
      "  6.90840090e-02+0.00000000e+00j  3.95962680e-02+0.00000000e+00j\n",
      "  1.66614842e-02+0.00000000e+00j  1.31963195e-02+0.00000000e+00j\n",
      "  6.28172756e-03+0.00000000e+00j  3.58913422e-03+0.00000000e+00j\n",
      "  2.67738097e-03+0.00000000e+00j  5.78927150e-04+0.00000000e+00j\n",
      "  1.56135245e-06+0.00000000e+00j  1.67729601e-07+0.00000000e+00j\n",
      " -2.83437488e-15+9.46607938e-16j -2.83437488e-15-9.46607938e-16j\n",
      "  2.79478689e-15+0.00000000e+00j  2.42396305e-15+0.00000000e+00j\n",
      " -1.97511462e-15+0.00000000e+00j -1.90160504e-15+0.00000000e+00j\n",
      " -1.73828225e-15+1.77847715e-16j -1.73828225e-15-1.77847715e-16j\n",
      "  2.04948379e-15+0.00000000e+00j  1.94604023e-15+0.00000000e+00j\n",
      "  1.76678029e-15+2.16548336e-16j  1.76678029e-15-2.16548336e-16j\n",
      " -1.41148074e-15+3.74247710e-17j -1.41148074e-15-3.74247710e-17j\n",
      "  1.48732899e-15+0.00000000e+00j  1.33265887e-15+0.00000000e+00j\n",
      "  1.17919802e-15+3.24002729e-17j  1.17919802e-15-3.24002729e-17j\n",
      "  1.06301310e-15+0.00000000e+00j -1.21039300e-15+0.00000000e+00j\n",
      " -1.14004911e-15+0.00000000e+00j -9.37589447e-16+0.00000000e+00j\n",
      "  5.87670082e-18+5.41376249e-16j  5.87670082e-18-5.41376249e-16j\n",
      " -7.62150809e-16+0.00000000e+00j -6.10351206e-16+0.00000000e+00j\n",
      " -4.24190779e-16+0.00000000e+00j -2.69752834e-16+5.42823079e-17j\n",
      " -2.69752834e-16-5.42823079e-17j  7.17375815e-16+8.53040399e-17j\n",
      "  7.17375815e-16-8.53040399e-17j  6.41225318e-16+0.00000000e+00j\n",
      "  5.62981528e-16+0.00000000e+00j  4.09359445e-16+0.00000000e+00j\n",
      "  2.26270951e-16+0.00000000e+00j  2.08273401e-16+0.00000000e+00j\n",
      "  1.19266196e-16+0.00000000e+00j -1.30351957e-17+0.00000000e+00j]\n"
     ]
    }
   ],
   "source": [
    "df_results=frame\n",
    "idx=0\n",
    "eigvals_full = np.frombuffer(eval(df_results['eigenvalues'][idx]), \n",
    "                             dtype=df_results['eigenvalues_dtype'][idx])\n",
    "# print(eigvals_full)\n",
    "nan_indices = np.argwhere(np.isnan(eigvals_full))\n",
    "eigvals_list = [None]*len(nan_indices)\n",
    "last_nan=-1\n",
    "for i in range(len(nan_indices)):\n",
    "    print(nan_indices[i][0])\n",
    "    eigvals_list[i] = eigvals_full[last_nan+1:nan_indices[i][0]]\n",
    "    last_nan=nan_indices[i][0]\n",
    "    \n",
    "for eig in eigvals_list:\n",
    "    print(eig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get back `cm_tr` and `cm_ts`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pk\n",
      "cm_tr:\n",
      "[[19  5]\n",
      " [ 1 72]]\n",
      " \n",
      "cm_ts:\n",
      "[[19  5]\n",
      " [12 62]]\n",
      "\n",
      "\n",
      "vc2c\n",
      "cm_tr:\n",
      "[[99  6]\n",
      " [12 38]]\n",
      " \n",
      "cm_ts:\n",
      "[[101   4]\n",
      " [ 19  31]]\n",
      "\n",
      "\n",
      "pk\n",
      "cm_tr:\n",
      "[[24  0]\n",
      " [ 0 73]]\n",
      " \n",
      "cm_ts:\n",
      "[[18  6]\n",
      " [ 5 69]]\n",
      "\n",
      "\n",
      "pk\n",
      "cm_tr:\n",
      "[[19  5]\n",
      " [ 1 72]]\n",
      " \n",
      "cm_ts:\n",
      "[[15  9]\n",
      " [ 2 72]]\n",
      "\n",
      "\n",
      "pk\n",
      "cm_tr:\n",
      "[[24  0]\n",
      " [ 0 73]]\n",
      " \n",
      "cm_ts:\n",
      "[[15  9]\n",
      " [ 5 69]]\n",
      "\n",
      "\n",
      "vc2c\n",
      "cm_tr:\n",
      "[[98  7]\n",
      " [13 37]]\n",
      " \n",
      "cm_ts:\n",
      "[[100   5]\n",
      " [ 21  29]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Dropbox/Mestrado/Pesquisa/Regional_Modeling/aux_func.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     temp_tr = np.frombuffer(eval( df_results['cm_tr'][i] ),\n\u001b[1;32m      4\u001b[0m                          dtype=df_results['cm_tr_dtype'][i])\n\u001b[1;32m      5\u001b[0m     temp_ts = np.frombuffer(eval( df_results['cm_ts'][i] ),\n",
      "\u001b[0;32m~/Dropbox/Mestrado/Pesquisa/Regional_Modeling/venv/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Mestrado/Pesquisa/Regional_Modeling/venv/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4721\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4722\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4723\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4724\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4725\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 6"
     ]
    }
   ],
   "source": [
    "for i in range(len(cases)):\n",
    "    print(df_results['dataset_name'][i])\n",
    "    temp_tr = np.frombuffer(eval( df_results['cm_tr'][i] ),\n",
    "                         dtype=df_results['cm_tr_dtype'][i])\n",
    "    temp_ts = np.frombuffer(eval( df_results['cm_ts'][i] ),\n",
    "                         dtype=df_results['cm_ts_dtype'][i])\n",
    "    print(\"cm_tr:\")\n",
    "    print(temp_tr.reshape( int(len(temp_tr)**(1/2)) ,-1))\n",
    "    print(\" \")\n",
    "    print(\"cm_ts:\")\n",
    "    print(temp_ts.reshape( int(len(temp_ts)**(1/2)) ,-1))\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
